{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"About","text":"<p>We are two economists and PhD candidates at the University of Z\u00fcrich. Through market.clearing, we hope to disseminate our research, share teaching materials on recent methodological advances, and engage in meaningful economic commentary. We are specialized in the fields of public economics, macroeconomics, and finance.</p> Frederik Bennhoff <p>                 My name is Frederik. I am a PhD candidate in Economics at the University of Z\u00fcrich, currently visiting LSE.                 </p> read more Igli Bajo <p>                 My name is Igli. I am a PhD candidate in Financial Economics at the University of Z\u00fcrich and the Swiss Finance Institute.                 </p> read more"},{"location":"fred/","title":"Frederik Bennhoff","text":"BioResearchCVTeaching <p> Frederik Bennhoff <p>PhD Candidate in Economics</p> <p>University of Zurich</p> <p> </p> <p>        I am a PhD candidate in Economics at the University of Zurich, currently visiting STICERD at the London School of Economics. I hold a MPhil in Economic Research from the University of Cambridge.          I specialize in public finance and in theory and computational methods in macroeconomics. In public finance, I study how end-of-life provisions of capital gains taxation distort portfolio choices. In macroeconomics, I study firm dynamics in macroeconomic models; an upcoming theoretical paper of mine with Igli Bajo and Alessandro Ferrari discusses how the productivity distribution of firms in an economy interacts with cleansing effects of recessions. Currently, I am working to apply sequence-space jacobian (Auclert, Rognlie &amp; Straub) solution methods in heterogeneous firm models, to learn more about how policy interacts with firms along a transitional path.        During 2019-2021 I was an economist (pre-doctoral fellow) at the Center for the Economics of Human Development at University of Chicago for Prof. James J. Heckman. My research from this period was mainly focused on labor economics and the evaluation of social programs.        </p> </p> <p></p> <p></p> <p> Macroeconomics for PhD Students University of Z\u00fcrich, Fall 2023 </p> <ul> <li>Role: Teaching Assistant</li> </ul> <p> Programming Practices for PhD Students University of Z\u00fcrich, Fall 2023 &amp; Fall 2024 </p> <ul> <li>Role: Instructor</li> <li>I have taught Python and R to PhD students. Since 2024, I have included a module Solving Economic Models, which you can access in the resources section of market.clearing </li> </ul> <p> Advanced Statistics for MSc. Students University of Z\u00fcrich, Fall 2022 </p> <ul> <li>Role: Teaching Assistant</li> </ul> <p></p>"},{"location":"fred/#research-statement","title":"Research Statement","text":"<p>I am interested in Macroeconomics and Public Finance. My macroeconomic research can be best summarized as firms in general equilibrium. I am interested in models which capture firm dynamics through the business cycle, and their interaction with fiscal and monetary policy. In public finance, I am working on capital gains and inter-generational taxation. Specifically, I am working to disentangle the effects of end-of-life provisions on portfolio choice and size of inheritances. My doctoral advisors are Florian Scheuer and Felix K\u00fcbler.</p>"},{"location":"fred/#publications","title":"Publications","text":""},{"location":"fred/#journal-articles","title":"Journal Articles","text":"<ol> <li> <p>The Dynastic Benefits of Early-Childhood Education: Participant Benefits and Family Spillovers [link]      Frederik H. Bennhoff, Jorge L. Garc\u00eda, Duncan Ermini Leaf  Journal of Human Capital 18 (1), 44-73</p> Summary <p> We demonstrate the social efficiency of investing in high-quality early-childhood education using newly collected data from the HighScope Perry Preschool Project. The data analyzed are the longest follow-up of any randomized early-childhood education program. Annual observations of participant outcomes up to midlife allow us to provide a cost-benefit analysis without relying on forecasts. Adult outcomes on the participants\u2019 children and siblings allow us to quantify spillover benefits. The program generates a benefit-cost ratio of 6.0 (p-value = .03). Spillover benefits increase this ratio to 7.5 (p-value = .00)</p> </li> </ol>"},{"location":"fred/#working-papers-ongoing-projects","title":"Working Papers &amp; Ongoing Projects","text":"<ol> <li> <p>Not-So-Cleaning Recessions      Igli Bajo, Frederik H. Bennhoff, Alessandro Ferrari  Draft in preparation.</p> Summary <p> Recessions are periods in which the least productive firms in the economy exit, and as the economy recovers, they are replaced by new and more productive entrants. These cleansing effects imply that business cycles generate improvements in the average firm productivity. We argue that this is not sufficient to induce long-run gains in GDP and welfare. We show that these are driven by the intensity of love-for-variety in households\u2019 preferences. If the household has CES preferences, recessions do not bring about any improvement in GDP and welfare. If the economy features more love-for-variety than CES, the social planner finds it optimal to subsidize economic activity in recessions to avoid firm exit. </p> </li> <li> <p>Capital Gains Taxation in Life and Death      Frederik H. Bennhoff, Florian Scheuer</p> Summary <p> End-of-life provisions determine the tax base of capital gains, when assets are bequeathed. Up until today, there existed no quantitative model to simulate the effects of changing end-of-life provisions, which jointly takes into account the reactions of households at two margins: (1) composition of the asset portfolio and (2) total size of the bequeathed estate. We build a novel, unifying framework in which one can simulate the consequences of different end-of-life provisions, bequest taxes and other income taxes simultaneously, thereby surpassing the current state-of-the-art at several margins. We do so by recasting the definition of asset portfolios, which makes the household problem theoretically and quantitatively tractable. </p> </li> </ol>"},{"location":"fred/#employment","title":"Employment","text":"<p>Doktorand (PhD) \u2014 University of Z\u00fcrich (UZH) Z\u00fcrich, CH PhD Student at the Zurich Graduate School of Economics Advisor: Prof. Florian Scheuer Aug. 2021 \u2013 today</p> <ul> <li>PhD Specializations: Recursive Methods, Macro-Finance, Public Finance (UZH), Advanced Machine Learning (ETH Z\u00fcrich)</li> <li>Workshops: Dynamic Structural Econometrics (Lausanne, 2023), Heterogeneous Agents Macroeconomics (by Auclert, Rognlie, Straub) (Frankfurt, 2024)</li> <li>Teaching: Macroeconomics (PhD), Programming Practices for Research Students (PhD), Advanced Statistics (MSc)</li> <li>Visiting doctoral researcher: London School of Economics (Aug. 2024 \u2013 Jan. 2025)</li> </ul> <p> Research Specialist \u2014 University of Chicago Chicago (IL), US Researcher at the Center of the Economics of Human Development Sep. 2019 \u2013 Jul. 2021</p> <ul> <li>Research specialist in public sector research projects for James J. Heckman at the University of Chicago.</li> <li>Statistical life-cycle evaluation of economic costs and benefits of a childhood education programme.</li> </ul> <p> Research Assistant \u2014 Center for Economic Studies (CESifo) Munich, DE Research assistant at the Center for Macroeconomics and Surveys (Prof. Andreas Peichl) Jul. 2018 \u2013 Sep. 2018</p> <p> Intern \u2014 KPMG Corporate Valuation Services and Consulting D\u00fcsseldorf/Cologne, DE Company evaluation projects Mar. 2016 \u2013 May 2016</p> <ul> <li>Evaluation of non-listed companies using DCF and WACC, joined KPMG HighQ talent pool.</li> </ul>"},{"location":"fred/#education","title":"Education","text":"<p>University of Cambridge (Trinity Hall) Cambridge, UK MPhil in Economic Research and Advanced Diploma in Economics Sep. 2017 \u2013 Jul. 2019</p> <ul> <li>Graduated MPhil with high distinction and award (GPA: 79/100), and Diploma best in class (1/25), with distinction and award (GPA: 74/100).</li> <li>Specialization in time-series statistics, MPhil dissertation in econometrics (Professor Oliver B. Linton)</li> </ul> <p> London School of Economics London, UK Mathematics; General Course Programme (Year abroad) Sep. 2016 \u2013 Jun. 2017 </p> <ul> <li>GPA: 79/100, distinction level.</li> </ul> <p> University of M\u00fcnster M\u00fcnster, DE BSc. in Business Administration Apr. 2013 \u2013 Sep. 2016</p> <ul> <li>Graduated top 5% of year, specialization &amp; thesis in asset pricing, corporate finance, financial accounting.</li> </ul>"},{"location":"fred/#awards-and-honors","title":"Awards and Honors","text":"<p>Doc.Mobility Studentship Z\u00fcrich, CH Award for an outstanding research project to be pursued abroad (LSE), CHF 30,000 2024</p> <p> Bateman Scholar Cambridge, UK Awarded by Trinity Hall (Cambridge college) for outstanding performance in an M.Phil. programme, \u00a3425 2021</p> <p> Awarded Cambridge Trust Scholar &amp; Trinity Hall Research Studentship Cambridge, UK Total award of \u00a333,115 (full stipend for MPhil degree, including cost of living) 2018\u20132019</p> <p> Stevenson Prize Cambridge, UK Award for best overall performance in the Advanced Diploma in Economics, \u00a3500 2018</p>"},{"location":"fred/#publications_1","title":"Publications","text":"<p>(See \u201cResearch\u201d)</p>"},{"location":"igli/","title":"Igli Bajo","text":"BioWorking PapersWork in ProgressTeaching <p> Igli Bajo <p>PhD Candidate in Finance</p> <p>University of Zurich and</p> <p>Swiss Finance Institute</p> <p> CV </p> <p>I\u2019m a PhD candidate in Financial Economics at the University of Zurich and the Swiss Finance Institute. I hold a MSc in Economics from the Barcelona School of Economics.         My research interests are macroeconomics, firm dynamics, and corporate finance. In my research, I study how information, financing, and household heterogeneity matter for firm dynamics, and how firm dynamics, in turn, impact long-run output and welfare.</p> </p> <p> Working Papers <ul> <li>\u201cNot-So-Cleansing Recessions\u201d, with Frederik Bennhoff and Alessandro Ferrari, 2024. Draft coming soon. Summary <p>Recessions are periods in which the least productive firms in the economy exit, and as the economy recovers, they are replaced by new and more productive entrants. These cleansing effects imply that business cycles generate improvements in the average firm productivity. We argue that this is not sufficient to induce long-run gains in GDP and welfare. We show that these are driven by the intensity of love-for-variety in households\u2019 preferences. If the household has CES preferences, recessions do not bring about any improvement in GDP and welfare. If the economy features more love-for-variety than CES, the social planner finds it optimal to subsidize economic activity in recessions to avoid firm exit.</p> </li> </ul> </p> <p> Work in Progress <ul> <li>\u201cWhy local stock markets matter\u201d, with Charles Parry.</li> <li>\u201cInformation disclosure and firm entry and exit\u201d, with Fulvia Oldrini.</li> <li>\u201cWealth inequality, concentration, and markups\u201d, with Jason Blunier.</li> </ul> </p> <p> Teaching <ul> <li>Instructor</li> </ul> <ul> <li>Python Programming, Powercoders Coding Academy, 2021-2022</li> </ul> <ul> <li>Teaching Assistant</li> </ul> <ul> <li>Macroeconomics for Research Students Part I (PhD), University of Zurich, Graduate, Fall 2024</li> <li>Real Analysis I, University of Zurich, Graduate, Fall 2023 and 2024</li> <li>Theses Supervision, University of Zurich, Graduate and Undergraduate, 2023-present</li> </ul> </p> <p></p>"},{"location":"sem/","title":"Solving Economic Models","text":"<p>This course is targeted at PhD students and practitioners, who would like to improve their coding skills in macroeconomics. The notebooks teach core scientific computing libraries (<code>numpy</code>, <code>numba</code>, <code>scipy</code>) and functional and object-oriented programming style by example. Examples are economic applications. As the course progresses, notebooks become increasingly focused on applying the coding concepts to state-of-the-art methods in computational econoimcs. </p> <p>The github page associated with the course can be found here: </p> <p> </p>"},{"location":"sem/1-numpy/","title":"Numpy Basics","text":"<p>To get started, install <code>numpy</code> using <code>pip</code> or <code>conda</code> package managers, or make sure that you have a recent version of numpy in your active Python environment.</p> <pre><code># as usual, import the module\nimport numpy as np # np is the standard name of numpy\n</code></pre>"},{"location":"sem/1-numpy/#1-arrays","title":"1. Arrays","text":"<p>The fundamental object in numpy are numpy arrays, <code>np.array</code>s. They are iterable objects which behave much like lists. They can have many dimensions. Create an array as follows. </p> <pre><code>a = np.array([1, 2, 3]) # create a numpy array from a list\nA = np.array([[1, 2, 3], [4, 5, 6]]) # create a 2-d array from a list with two lists\nprint(\"a = \", a, '\\n', \"A = \", A, sep=\"\") # extra bit of python: sep argument in print function defines the separating string\n</code></pre> <p>You cannot create numpy arrays from any kind of list. If your list contains integers and floats, integers will be \u2018up-cast\u2019 to the float datatype. If your list contains string, all else will be up-cast to string. What if it contains another datatype, like a list? Brace for an error.</p> <pre><code>print(\"an up-cast array to float: \", np.array([1.2, 1, 3]))\nprint(\"an up-cast array to str: \", np.array(['1.2', 1, 3]))\n\n#print(\"something illegal: \", np.array([1, 2, 3, [1, 2]]) ) # this will throw an error\n</code></pre> <p>Exercise: Use the \u2018dtype\u2019 argument to save the list <code>[1.9, 1, 3]</code> as a numpy array consisting of str/float/int/bool. What do you notice in the <code>int</code>-case? In the <code>bool</code>-case?</p> <pre><code>print(\"Begin with list: \", [1.9, 1, 3])\nprint(\"forced data-type to str: \", np.array([1.9, 1, 3], dtype=int))\nprint(\"forced data-type to int: \", np.array([1.9, 1, 3], dtype=str))\nprint(\"forced data-type to int: \", np.array([1.9, 1, 3], dtype=bool))\n</code></pre> <p>For Reference: Data Types</p> Data type Description <code>bool_</code> Boolean (True or False) stored as a byte <code>int_</code> Default integer type (same as C <code>long</code>; normally either <code>int64</code> or <code>int32</code>) <code>intc</code> Identical to C <code>int</code> (normally <code>int32</code> or <code>int64</code>) <code>intp</code> Integer used for indexing (same as C <code>ssize_t</code>; normally either <code>int32</code> or <code>int64</code>) <code>int8</code> Byte (-128 to 127) <code>int16</code> Integer (-32768 to 32767) <code>int32</code> Integer (-2147483648 to 2147483647) <code>int64</code> Integer (-9223372036854775808 to 9223372036854775807) <code>uint8</code> Unsigned integer (0 to 255) <code>uint16</code> Unsigned integer (0 to 65535) <code>uint32</code> Unsigned integer (0 to 4294967295) <code>uint64</code> Unsigned integer (0 to 18446744073709551615) <code>float_</code> Shorthand for <code>float64</code>. <code>float16</code> Half precision float: sign bit, 5 bits exponent, 10 bits mantissa <code>float32</code> Single precision float: sign bit, 8 bits exponent, 23 bits mantissa <code>float64</code> Double precision float: sign bit, 11 bits exponent, 52 bits mantissa <code>complex_</code> Shorthand for <code>complex128</code>. <code>complex64</code> Complex number, represented by two 32-bit floats <code>complex128</code> Complex number, represented by two 64-bit floats <p>Source: Jake VanderPlas (2016), Python Data Science Handbook Essential Tools for Working with Data, O\u2019Reilly Media.</p> <p>Exercise: Explore the commands <code>np.zeros</code>, <code>np.ones</code>, <code>np.full</code> as follows:</p> <ul> <li>instantiate a <code>(2, 3, 4)</code> array with each of the commands</li> <li>select element <code>[0, 0, 2]</code> for each object and set its value to <code>100</code></li> <li>print each object.</li> </ul> <pre><code>A = np.zeros(shape=(2, 3, 4))\nB = np.ones(shape=(2, 3, 4))\nC = np.full((2, 3, 4), 4) # different syntax for full\n\nfor x in [A, B, C]:\n    x[0,0,2] = 100\n\n# extra bit of python syntax:\n# \"abc\" * 10 produces a string which repeats \"abc\" 10 times.\nprint(A, \"\\n \", \"-\" * 70,  \"\\n\",  B, \"\\n \", \"-\" * 70,  \"\\n\", C) \n</code></pre> <p>Arrays have methods and attributes associated with them. An attribute is <code>dtype</code>, another attribute is <code>shape</code>. We will cover methods below.</p> <pre><code>print(A.dtype, \"\\n\", A.shape, sep=\"\")\n</code></pre> <p>The command <code>np.empty</code> instantiates an empty array, the command <code>np.empty_like</code> instantiates some array which has the same dimension and data type as a given array. Empty arrays are computationally less wasteful to produce than e.g. <code>np.zeros</code>, if one replaces the elements in some process anyways. The print function called on an empty array produces mostly garbage, don\u2019t worry about this.</p> <p>The following two are equivalent: <code>np.empty(shape = A.shape)</code> and <code>np.empty_like(A)</code></p> <pre><code>print(np.empty(shape = A.shape).dtype)\nprint(np.empty_like(A).shape)\nprint(np.empty((2, 3)))\n</code></pre>"},{"location":"sem/1-numpy/#array-indexing","title":"Array Indexing","text":"<p>Indexing arrays is easy and best done by example. We can select elements by regular indexing and Boolean indexing:</p> <pre><code>A = np.matrix([[1, 2, 3], [4, 5, 6]]) # a matrix is basically a (n,m)-array\nprint(A)\n\n# indexing\nA[1,0] = A[1,0]*10 # multiply element [0,1] by 10\nprint(A)\n\n# boolean mask\nA[A &lt; 3.2] = 0 # replace all elements &lt; 3.2 by 0\nprint(A)\n</code></pre> <p>Note that above we created a <code>np.matrix</code>, which is in many ways similar to a numpy array. However, it has some methods associated with it which are specific for linear algebra. Additionally, some operators are interpreted differently. The asterisk <code>*</code> defines element-wise multiplication for arrays, but always refers to matrix-multiplication for matrices. Furthermore, when making a boolean comparison where only matrices are involved, the returned object will be a boolean matrix and also have matrix operations associated to it.</p> <p>Note that the Boolean comparison is done element-wise on the elements of the matrix (here, <code>A</code>):</p> <pre><code>A &gt; 10\n</code></pre> <p>Finally, we consider the difference between views and copies:</p> <p>If we want to obtain a true copy of some array <code>A</code>, we need to enclose it in <code>np.copy</code>. Otherwise, we will obtain a view, which if called links back to the original object <code>A</code>.</p> <pre><code>A_copy = np.copy(A) # use the copy function to make A_copy, which DOES NOT change if we change A\n\n# however, this LHS variable will 'point' to the elements of A_copy\n# if we change stuff in A_copy[0,:], then the_first_row_of_A_copy changes\nthe_first_row_of_A_copy = A_copy[0,:] \nprint(the_first_row_of_A_copy)\n\nA_copy[0,:] = np.arange(1, 4, 1) # np.arange(start, end, step) creates a sequence from start to (excluding) end, in 'step'-size\nprint(the_first_row_of_A_copy)\n</code></pre> <p>We can slice arrays in any dimensions using the usual <code>i:j:k</code> syntax: select every <code>k</code>th element from <code>i</code> to <code>j</code>.</p> <p>Exercise:  - Write a function <code>make_shift_mat</code> which takes a number n and outputs the n by n shift matrix </p>  \\begin{align*} S_n =  \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{pmatrix} \\end{align*}  <ul> <li>Create another function <code>make_nilpotent_mat</code> which takes as argument a list of integers <code>[n_1, n_2, ..., n_m]</code> and outputs the a matrix which has S_{n_1}, ..., S_{n_m} on the diagonal and is 0 in all other entries.</li> <li>Then create a function <code>get_np_index</code> which takes in a nilpotent matrix N and calculates the (least) k, for which N^k = \\mathbf{0} (the zero matrix)</li> </ul> <p>HINT: To calculate a matrix product of two arrays <code>A, B</code> of conforming dimension, use <code>A@B</code>.</p> <p>Solution:</p> <pre><code># a solution with a loop\ndef make_shift_mat(n):\n    A = np.zeros([n,n])\n    for i in range(n-1):\n        A[i,i+1] = 1\n    return A\n\n# an alternative way with slicing\ndef make_shift_mat(n):\n    A = np.zeros((n,n))\n    A[np.arange(n-1), np.arange(1,n)] = 1\n\n    return A\n\nmake_shift_mat(8)\n\ndef make_nilpotent_mat(dim_list): \n    B = np.zeros((sum(dim_list),sum(dim_list)))\n    start = 0\n    for n in dim_list:\n        end = start + n\n        B[start:end, start:end] = make_shift_mat(n)\n        start += n\n    return B\n\n\ndef get_np_index(M, max_iter=10_000):\n    M_copy = M.copy()\n    counter = 0\n    while not np.all(M_copy == 0):\n        counter += 1\n        M_copy = M_copy @ M\n\n        if counter &gt;= max_iter:\n            print(\"NO NILPINDEX FOUND\")\n            return None      \n\n    print(\"NILPINDEX\", counter+2)\n    return counter+2\n\n\ndef get_np_index(M, max_iter=10_000):\n    M_copy = M.copy()\n\n    for i in range(max_iter):\n\n        M_copy = M_copy @ M\n\n        if np.all(M_copy == 0):    \n            print(\"NILPINDEX\", i+2)\n            return i+2\n\n    print(\"NO NILPINDEX FOUND\")\n    return None      \n\n\nM = make_nilpotent_mat([6, 2, 5])\n\nget_np_index(M)\n# check that it works\nM @ M @ M @ M @ M @ M \n</code></pre>"},{"location":"sem/1-numpy/#functions-to-make-arrays","title":"Functions to Make Arrays","text":"<p>Numpy has a few handy built-in methods to create arrays of different kinds. <code>np.ones</code> and <code>np.zeros</code> are such. Now we look at two more: <code>np.linspace</code> and <code>np.logspace</code>. </p> <pre><code>np.linspace(start=0., stop=100, num=20) # create a grid from 0 to 100 with 20 grid points\nnp.logspace(start=np.log(1), stop=np.log(100), num=20, base = np.e) # create a grid from 0 to 100 with 20 grid points but in logs\n\n# the latter is equivalent to \nlower = np.log(1) # np.log( . ) is the natural logarithm\nupper = np.log(100)\nnp.exp(np.linspace(lower, upper, 20)) # use broadcasting here, see next section\n</code></pre> <p>The function <code>np.arange</code> is very similar to the native <code>range()</code> function:</p> <pre><code>np.arange(start=1, stop=20, step=3)\n</code></pre>"},{"location":"sem/1-numpy/#2-computations-and-broadcasting","title":"2. Computations and Broadcasting","text":"<p>One can do pretty much all basic computations on <code>np.array</code>s and these will be applied elementwise. (These computations are also called vectorized functions.) </p> <p>Let\u2019s also consider some functions coming with numpy: <code>np.exp, np.log, np.abs</code>.</p> <pre><code>a = np.arange(1,10,2)\nprint(a)\n\n# multiplication / addition with a scalar\nprint(a * 10)\nprint(a + 10)\n\n# exponential function, log function\nprint(np.exp(a))\nprint(np.log(a))\n\nb = np.array([-1,0,1])\nprint(np.abs(b))\n</code></pre> <p>We can add two <code>np.array</code>s together element-wise in the obvious fashion. This is called \u2018broadcasting\u2019. My advice: While useful, try to avoid situations in which non-obvious broadcasting things can happen.</p> <p>Instead of addition, we can perform above computations with multiplication <code>*</code> or exponentiation <code>**</code>, too.</p> <p>An interesting way to leverage broadcasting is to use <code>np.newaxis</code>, which creates an additional yet empty dimension to an array. For example, let <code>a = np.array([1, 2, 3])</code>. Then <code>a[:, np.newaxis]</code> has shape <code>(3, 1)</code> and <code>a[np.newaxis, :]</code> has shape <code>(1, 3)</code>. What does the following broadcasting procedure yield: <code>B = a[np.newaxis, :] + a[:, np.newaxis]</code>?</p> <p>Think it through. If you repeat <code>a[np.newaxis, :]</code> on the second axis you get a <code>(3,3)</code> array. Same if you repeat <code>a[:, np.newaxis]</code> thrice on the first axis. Then, you add the two resulting matrices elementwise. That\u2019s basically an outer product!</p> <pre><code>a = np.array([1, 2, 3])\n\nprint(a**2, \"\\n\")\n\nprint(a[:, np.newaxis].shape, \"\\n\")\n\nprint(a[np.newaxis, :] + a[:, np.newaxis], \"\\n\")\n\nprint(a[np.newaxis, :] ** a[:, np.newaxis])\n</code></pre> <p>Rules of Broadcasting</p> <p>Now we have seen Broadcasting in action, let\u2019s consider the rules that NumPy us using to determine how it operates on two arrays:</p> <ul> <li>Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side.</li> <li>Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape.</li> <li>Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised.</li> </ul> <p>Consider an operation on <code>matrix</code> and <code>array</code>. The shape of each is:</p> <ul> <li><code>matrix.shape = (2, 3)</code></li> <li><code>array.shape = (3,)</code></li> </ul> <p>Rule 1 tells us that <code>array</code> has fewer dimensions, so we pad it on the left with ones:</p> <ul> <li><code>matrix.shape -&gt; (2, 3)</code></li> <li><code>array.shape -&gt; (1, 3)</code></li> </ul> <p>Rule 2, tells us the first dimension disagrees, so we stretch itnsion to match:</p> <ul> <li><code>matrix.shape -&gt; (2, 3)</code></li> <li><code>array.shape -&gt; (2, 3)</code></li> </ul> <p>Now the shapes match, and we see the output of a ufunc operation will be <code>(2, 3)</code>:</p> <p>(Source PP4RS, 2023)</p> <p>Above, we have seen rule 2 in action. Now look at rule 1:</p> <pre><code># Example of padding\n\na = np.matrix([2, 2, 2])\nb = np.array([1, 2, 3])\n\nprint(a.shape, b.shape) # look at the shape of array and matrix\n\n(a + 0 * b).shape # look at the shape of the broadcasted result\n</code></pre>"},{"location":"sem/1-numpy/#array-methods","title":"Array Methods","text":"<p>We spent a minute on useful methods (=functions) associated with <code>numpy</code> objects. First, we create an array. Then we <code>reshape</code>the array. Then we consider some <code>sum</code>, <code>max</code>, <code>min</code>, <code>mean</code> commands.</p> <pre><code>a = np.arange(1, 100, 3.1234) \n\nprint(a.shape) # well, lets make this a (4, 8) array. \n# the reshape method will first fill up the columns, for row 1, then for row 2, etc until all rows are filled\n# an analogous reshaping occurs when looping through multiple dimensions.\n\na = np.reshape(a, (4, 8))\nprint(a.shape)\n\n# global max\nprint(a.max())\n\n# row max: take max over the first dimension\nprint(a.max(axis=0))\n\n# column min: take min over the second dimension\nprint(a.min(axis=1))\n\n# the mean and sum methods work completely analogously.\n</code></pre> <p>Note: instead of using <code>np.copy(a)</code>, we can always write <code>a.copy()</code> since <code>copy()</code> is a method of the numpy object.</p>"},{"location":"sem/1-numpy/#matrix-computations","title":"Matrix Computations","text":"<p>Of course we can use matrix algebra with <code>numpy</code>.</p> <pre><code># transpose of A\nA.transpose()\n\n# multiply A to A'\nA @ A.transpose()\n\nnp.dot(A, A.transpose()) # alternatively, use np.dot instead of @\n</code></pre>"},{"location":"sem/1-numpy/#3-the-nprandom-module","title":"3. The <code>np.random</code> Module","text":"<p>The <code>np.random</code> module is a collection of functions able to generate random variables and fill array with those. For instance, <code>np.random.uniform</code> generates samples drawn from the uniform distribution. Can you guess what <code>np.random.randint</code> will give us?</p> <pre><code>u = np.random.uniform(low=0, high=1, size=100)\n\n# random matrix\nu = u.reshape(10,10)\n\n# random matrix with integers\nu_int = np.random.randint(low=0, high=101, size=100).reshape(10,10)\n</code></pre> <p>Exercise:</p> <p>Write the function <code>demean</code> which takes two inputs, a numpy array <code>A</code> and <code>axis</code> defaulting to <code>None</code>. If <code>axis</code> is equal to the default, the function calculate the mean across all elements of <code>A</code> and subtract this mean from each element. If <code>axis = i</code>, then the function calculates the means of <code>A</code> along that axis and only demeans in this direction. For example, it calculates a_{i, j, k, l} - \\bar a_{i, j, \\cdot, l} if <code>axis = 2</code>. (This means the third dimension is used to calculate means.)</p> <pre><code>def demean(A, axis=None):\n    A_mean = A.mean(axis=axis, keepdims = True)\n    return A - A_mean # broadcasting rule 2 is our friend here\n\ndemean(A, axis=1) \n</code></pre>"},{"location":"sem/1-numpy/#4-other-exercises","title":"4. Other Exercises","text":"<p>Exercise: Discretizing the asset space</p> <p>Consider the situation in which we want to create an asset grid for the households in your favorite macro-model. Households choose assets from [\\underline a, \\infty), need to discretize this grid. Idea: there should be more grid points around \\underline a, because policy function is more nonlinear here. But we also want some points for high a. Solution: double exponential transformation of uniformly spaced grid:</p> a_i = \\underline a + e^{e^{u_i} - 1} - 1, u_i \\in [0, \\bar u] <p>Write a function <code>discretize_assets(amin, amax, n_a)</code> which outputs an asset grid ranging from <code>a_min</code> to <code>a_max</code> with <code>n_a</code> grid points, where a_i is determined by the formula above.</p> <pre><code># SOLUTION:\n\n# write a function which discretizes the asset space\ndef discretize_assets(amin, amax, n_a):\n    # find ubar \n    ubar = np.log(np.log(amax - amin + 1)+1)\n    # make linar grid for the u's\n    grid_u = np.linspace(0,ubar, n_a)\n    # transform back to a\n    grid_a = amin + np.exp(np.exp(grid_u)-1)-1\n    return grid_a\n\ngrid_a = discretize_assets(0, 10_000, 50)\n\n# visualize the grid\nimport matplotlib.pyplot as plt\n# some useful plot defaults\nplt.rcParams.update({'font.size' : 10, 'lines.linewidth' : 1.5, 'figure.figsize' : (4,3)})\n# scatter plot of the grid\nplt.scatter(grid_a, np.repeat(1, len(grid_a)), marker='|')\n</code></pre>"},{"location":"sem/2-numba/","title":"Numba","text":"<p>Numba is a library which can speed up computations, especially loops, enormously. We look at two use cases: just-in-time compilation and parallelization.</p>"},{"location":"sem/2-numba/#1-just-in-time-jit-compilation","title":"1. Just-In-Time (JIT) compilation","text":"<p>JIT means that a function you write in Python is not evaluated in its Python code. Rather, it is translated into very fast machine code the first time you run it (\u2018compilation\u2019 step). Function calls are accelerated after you run it for the first time. This works well for Python functions using only base-Python or numpy code, especially if they contain loops. However, not all functions are JITable. Numba will throw an error upon execution in such cases.</p> <p>We JIT a function by including the decorator <code>@jit</code> in front of the function definition.</p> <p>Whenever we can JIT a loop, the speed of execution will be comparable to high-speed, vectorized calculations.</p> <pre><code>from numba import jit\nimport numpy as np\nimport matplotlib.pyplot as plt \n</code></pre> <p>Example 1:  A function to sum the entries of a matrix.</p> <pre><code>def sum2d_slow(arr):\n    M, N = arr.shape\n    result = 0.0\n    for i in range(M):\n        for j in range(N):\n            result += arr[i, j]\n    return result\n\n@jit(nopython=True)\ndef sum2d(arr):\n    M, N = arr.shape\n    result = 0.0\n    for i in range(M):\n        for j in range(N):\n            result += arr[i, j]\n    return result\n\narr = np.random.random((1000, 1000))\nprint(sum2d(arr))\n</code></pre> <pre><code>%timeit sum2d(arr)\n%timeit sum2d_slow(arr)\n</code></pre> <p>Example 2: A function to calculate the nth entry of the Fibonacci sequence x_n = x_{n-1} + x_{n-2} with x_{1}, x_2 = 1.</p> <pre><code>@jit(nopython=True)\ndef fibonacci(n):\n    if n &lt;= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\nprint(fibonacci(10))\n</code></pre> <p>Example 3: A custom function to perform matrix multiplication.</p> <pre><code>@jit(nopython=True)\ndef matmul(A, B):\n    M, K = A.shape\n    K, N = B.shape\n    C = np.empty((M, N))\n    for i in range(M):\n        for j in range(N):\n            for k in range(K):\n                C[i, j] += A[i, k] * B[k, j]\n    return C\n\nA = np.random.random((5, 5))\nB = np.random.random((5, 5))\nprint(matmul(A, B))\n</code></pre> <p>Exercise: Time the functions in all examples and compare the runtime to a non-JIT\u2019ed version.</p>"},{"location":"sem/2-numba/#2-parallelization","title":"2. Parallelization","text":"<p>We can make use of out computers\u2019 ability to perform multiple calculations at the same time with <code>numba</code>, too. To parallelize loops, the following two simple steps are needed:</p> <ol> <li>The decorator becomes <code>@jit(nopython=True, parallel=True)</code></li> <li>All loops to be parallelized are called with <code>prange</code> objects, instead of the familiar <code>range</code>.</li> </ol> <p>Parallelization works best in scenarios where tasks can be divided into independent units of work that can be executed concurrently without dependencies. This includes operations on large datasets, such as matrix multiplications, image processing, and simulations like Monte Carlo methods or random walks. It is particularly effective when the workload can be evenly distributed across multiple processors, minimizing the need for synchronization and communication between tasks. By leveraging parallelization, significant performance improvements can be achieved, especially in computationally intensive applications.</p> <p>Here is an example for a parallelized matrix-multiplication:</p> <pre><code>from numba import prange\n\n@jit(nopython=True, parallel=True)\ndef parallel_matmul(A, B):\n    M, K = A.shape\n    K, N = B.shape\n    C = np.zeros((M, N))\n    for i in prange(M):\n        for j in prange(N):\n            for k in range(K):\n                C[i, j] += A[i, k] * B[k, j]\n    return C\n\nA = np.random.random((5, 5))\nB = np.random.random((5, 5))\nprint(parallel_matmul(A, B))\n</code></pre> <pre><code>%timeit matmul(A, B)\n%timeit parallel_matmul(A, B)\n</code></pre> <p>Exercise: Parallel simulation of a brownian motion. The formula for a Geometric Brownian Motion (GBM) is given by:</p>  dS_t = \\mu S_t \\, dt + \\sigma S_t \\, dW_t  <p>where: - S_t is the stock price at time t - \\mu is the drift coefficient (expected return) - \\sigma is the volatility coefficient (standard deviation of returns) - dW_t is a Wiener process or Brownian motion</p> <p>In its discrete form, the GBM can be expressed as:</p>  S_{t+\\Delta t} = S_t \\exp \\left( \\left( \\mu - \\frac{\\sigma^2}{2} \\right) \\Delta t + \\sigma \\sqrt{\\Delta t} \\, Z_t \\right)  <p>where:</p> <ul> <li>\\Delta t is the time increment</li> <li>Z_t is a standard normal random variable</li> </ul> <p>This formula is commonly used in financial mathematics to model stock prices and other financial instruments.</p> <p>Write a parallelized function <code>GBM_sim</code> which generates returns <code>N</code> brownian motion simulations of duration <code>T</code> each, and takes in the needed parameters. Which loops are easily parallelized and which are not?</p> <pre><code>@jit(nopython=True, parallel=True)\ndef GBM_sim(S0, mu, sigma, deltaT, T, N):\n\n    deltaT_sqrt = deltaT**0.5 # pre-compute this guy \n    increments_num = int(np.floor(T/deltaT) + 1) # need integer number of increments\n    log_GBMs = np.empty((N,increments_num))\n\n    for n in prange(N):\n        log_GBMs[n, 0] = np.log(S0)\n        for i in range(1, increments_num):\n            log_GBMs[n, i] = log_GBMs[n, i-1] + (mu - sigma**2 / 2) * deltaT + sigma * deltaT_sqrt * np.random.normal()\n\n    GBMs = np.exp(log_GBMs)\n    return GBMs\n\nGBMs = GBM_sim(S0 = 1, mu = 0.08, sigma = 1, deltaT = 0.001, T = 1, N = 5)\n</code></pre> <p>Let\u2019s plot some of these Brownian motions.</p> <pre><code># Define shades of dark grey and different linestyles\ncolors = ['#2f2f2f', '#3f3f3f', '#4f4f4f', '#5f5f5f', '#6f6f6f', '#7f7f7f', '#8f8f8f', '#9f9f9f', '#afafaf', '#bfbfbf']\nlinestyles = ['-', '--', '-.', ':', '-', '--', '-.', ':', '-', '--']\n\nfor i in range(GBMs.shape[0]):\n    plt.plot(GBMs[i], color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)])\n\nplt.ylabel(r'$S_t$')\nplt.xlabel(r't')\nplt.show()\n</code></pre> <p>Let\u2019s time the function:</p> <pre><code>%%timeit\nGBM_sim(S0 = 1, mu = 0.08, sigma = 1, deltaT = 0.001, T = 1, N = 5000)\n</code></pre> <p>Exercise: Write a function <code>call_payout(price, strike)</code>, which gives you the value of a call option at maturity if the price of the underlying are given in <code>prices</code> and you have the right (but not the obligation) to buy the underlying for the strike price <code>strike</code>.</p> <pre><code>def call_payout(prices, strike):\n    payout = (prices - strike)\n    payout[payout &lt; 0] = 0\n    return payout\n</code></pre> <p>Exercise: Write a function which simulates the mean expected payoffs of a call option on an asset initially priced at S_0 after T periods. Simulate the function for <code>100_000</code> paths per GBM run, and across \\sigma \\in \\{0.5, 1, 2\\}</p> <pre><code>def sim_payoff(S0 = 1, mu = 0.08, sigma = 1, deltaT = 0.001, T = 1, N = 10000, discount_rate=0.03):\n    GBMs = GBM_sim(S0, mu, sigma, deltaT, T, N)\n    mean_payoff = call_payout(GBMs, 1.0).mean(axis=0)\n    mean_payoff = mean_payoff / (1+discount_rate*deltaT) ** np.arange(T/deltaT+1)\n    return mean_payoff\n\nmean_payoff = sim_payoff(S0 = 1, mu = 0.08, sigma = 0.5, deltaT = 0.001, T = 1, N = 100_000, discount_rate=0.08)\nmean_payoff = mean_payoff[:,np.newaxis]\nfor i, sigma in enumerate((1., 2.)):\n    mean_payoff = np.append(mean_payoff, sim_payoff(S0 = 1, mu = 0.08, sigma = sigma, deltaT = 0.001, T = 1, N = 100_000, discount_rate=0.08)[:, np.newaxis], axis=1)\n</code></pre> <pre><code>for i, sigma in enumerate((0.5, 1., 2.)):\n    plt.plot(np.linspace(0, 1, len(mean_payoff[:, i])), mean_payoff[:, i] , label = f\"$\\sigma = {sigma}$\")\nplt.ylabel(r'$E[payoff_t] * (1+r \\Delta t)^{-t/\\Delta t}$')\nplt.xlabel(r'Number of years, $t$')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"sem/3-application_stoch_proc/","title":"Application: Stochastic Processes","text":"<p>In this notebook we put some of the techniques we just learned to good use. We will: - write a function to simulate ARMA(p,q) processes - write a function to simulate Markov-processes - Introduce the Rouwenhorst method to approximate an AR(1) process using a Markov process on a finite grid.</p> <p>Additionally, we write our own class <code>markov</code> with simulation and calibration methods.</p>"},{"location":"sem/3-application_stoch_proc/#armapq","title":"ARMA(p,q)","text":"<p>Recall the definition of an ARMA(p,q) process as  $$ y_t = \\alpha_0 + \\sum_{j = 1}^p \\alpha_j L^j y_t + \\sum_{j = 1}^q \\beta_j L^j \\varepsilon_t + \\varepsilon_t$$ where L is the lag-operator and \\varepsilon_t\\sim_{i.i.d.} \\Phi(\\sigma, 0) (we assume a normal distribution on the errors).</p> <p>Let\u2019s write a function which takes in a dictionary holding \\alpha_0, \\underline{\\alpha}, \\underline{\\beta} and \\sigma to simulate the corresponding ARMA process.</p> <pre><code>import numpy as np\nfrom numba import prange, jit\n</code></pre> <pre><code>arma_dict = {\n    'alpha_0': 0.,\n    'alpha': np.array([0.2, 0.4, 0.1]),  \n    'beta': np.array([0.1]),\n    'sigma': 1\n}\n\ndef sim_arma(arma_dict, T):\n    p = len(arma_dict['alpha'])\n    q = len(arma_dict['beta'])\n    alpha = np.flip(arma_dict['alpha']) # reverse the vectors to make sure a_j is multiplied to y_t-j in np.vdot \n    beta = np.flip(arma_dict['beta'])\n\n    y = np.empty(1000+T) # 1000 burn-in draws\n    eps = np.random.normal(0., arma_dict['sigma'], T+1000)\n    y[0:max(p,q)] = eps[0:max(p,q)]  \n    for i in np.arange(max(p,q), T+1000):\n        y[i] = np.vdot(y[i-(p):(i)], alpha) + np.vdot(eps[i-(q):(i)], beta)\n    return y[-T:]\n</code></pre> <pre><code>arma_ts = sim_arma(arma_dict, 250)\n</code></pre> <p>Let\u2019s write a function to plot the time series.</p> <pre><code>import matplotlib.pyplot as plt\n\ndef plot_time_series(time_series, arma_dict, xlabel='Time', ylabel='Value'):\n    \"\"\"\n    Plots the given time series.\n\n    Parameters:\n    time_series (array-like): The time series data to plot.\n    title (str): The title of the plot.\n    xlabel (str): The label for the x-axis.\n    ylabel (str): The label for the y-axis.\n    \"\"\"\n    title='Time Series ARMA(' + str(len(arma_dict['alpha'])) + \", \" + str(len(arma_dict['beta'])) + \")\"\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(time_series, color='blue', linestyle='-', linewidth=1.)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    plt.show()\n</code></pre> <pre><code># Plot the time series\nplot_time_series(arma_ts, arma_dict)\n</code></pre> <p></p> <p>Recall now that a given ARMA(p,q) is stationary if and only if all roots of the characteristic polynomial on the AR-part,  $$ 1 - L^1 \\alpha_1 - \u2026 - L^q \\alpha_q $$ are outside the (complex) unit circle. We write a function to check this. To do so, we use <code>np.roots(p)</code>, which return the roots of a polynomial with coefficients given in <code>p</code>.</p> <pre><code>def is_stable(alpha):\n    coefs = np.concatenate([[1], -alpha])[::-1] # [::-1] reverses an array\n    print(\"The lag polynomial to check is:\")\n    print(\"1 -\", \" - \".join([f\"{a}*L^{i}\" for i, a in enumerate(alpha)]))\n    print(\"The roots are:\")\n    roots = np.roots(coefs)\n    for root in roots:\n        print(f\" {root:.2f}\")\n\n    # Check if all roots have modulus &gt; 1\n    are_roots_outside_unit_circle = np.all(np.abs(roots) &gt; 1)\n    if are_roots_outside_unit_circle : \n        print(\"\\nThe process is stable.\")\n    else :\n        print(\"\\nThe process is unstable.\")\n    return are_roots_outside_unit_circle\n</code></pre> <p>Let\u2019s try it!</p> <pre><code>is_stable(arma_dict['alpha'])\n</code></pre> <pre><code>The lag polynomial to check is:\n1 - 0.2*L^0 - 0.4*L^1 - 0.1*L^2\nThe roots are:\n -2.60+1.23j\n -2.60-1.23j\n 1.21+0.00j\n\nThe process is stable.\n\n\n\n\n\nTrue\n</code></pre>"},{"location":"sem/3-application_stoch_proc/#a-taster-of-oop","title":"A Taster of OOP","text":"<p>In Python, classes are a fundamental building block of object-oriented programming (OOP). A class is a blueprint for creating objects (instances), which can have attributes (variables) and methods (functions) associated with them.</p> <p>Basic Structure of a Class</p> <p>Here\u2019s a simple example to demonstrate the structure of a class in Python:</p> <pre><code>class Dog:\n    # Class attribute (shared by all instances)\n    species = \"Canis familiaris\"\n\n    # The initializer method (also called the constructor)\n    def __init__(self, name, age):\n        # Instance attributes\n        self.name = name\n        self.age = age\n\n    # Instance method\n    def bark(self):\n        return f\"{self.name} says woof!\"\n\n    # Another instance method\n    def get_human_years(self):\n        return self.age * 7\n\n# Creating instances (objects) of the class\ndog1 = Dog(\"Buddy\", 5)\ndog2 = Dog(\"Lucy\", 3)\n\n# Accessing attributes and methods\nprint(dog1.name)  # Output: Buddy\nprint(dog1.bark())  # Output: Buddy says woof!\nprint(dog2.get_human_years())  # Output: 21\n</code></pre> <pre><code>Buddy\nBuddy says woof!\n21\n</code></pre> <p>Key Concepts:</p> <ol> <li> <p>Class Definition:</p> <ul> <li>A class is defined using the <code>class</code> keyword followed by the class name and a colon.</li> <li>By convention, class names are written in CamelCase (e.g., <code>Dog</code>).</li> </ul> </li> <li> <p>Attributes:</p> <ul> <li>Class Attributes: These are shared across all instances of the class. In the example, <code>species</code> is a class attribute.</li> <li>Instance Attributes: These are specific to each instance of the class. They are defined inside the <code>__init__</code> method (constructor). In the example, <code>name</code> and <code>age</code> are instance attributes.</li> </ul> </li> <li> <p>Methods:</p> <ul> <li>Methods are functions defined within a class that operate on instances of the class.</li> <li>Instance Methods: These take <code>self</code> as the first parameter, which refers to the instance calling the method. For example, <code>bark</code> and <code>get_human_years</code> are instance methods in the <code>Dog</code> class.</li> <li>The <code>__init__</code> method is a special method called automatically when a new instance of the class is created. It is used to initialize the instance\u2019s attributes.</li> </ul> </li> <li> <p>Creating Objects:</p> <ul> <li>Objects (instances) are created by calling the class as if it were a function, passing any arguments required by the <code>__init__</code> method.</li> <li>For example, <code>dog1 = Dog(\"Buddy\", 5)</code> creates an instance of the <code>Dog</code> class with <code>name</code> as <code>\"Buddy\"</code> and <code>age</code> as <code>5</code>.</li> </ul> </li> <li> <p>Accessing Attributes and Methods:</p> <ul> <li>Instance attributes and methods are accessed using dot notation (e.g., <code>dog1.name</code>, <code>dog1.bark()</code>).</li> <li>Class attributes can be accessed directly via the class name or through any instance (e.g., <code>Dog.species</code> or <code>dog1.species</code>).</li> </ul> </li> </ol> <p>After we got the basic stuff out of the way, let\u2019s write a class for ARMA(p, q) processes. We call the class <code>arma</code> and have the following desiderata:</p> <ul> <li>The class is initialized with the four inputs making up <code>arma_dict</code></li> <li>It has a method allowing to simulate a the process with the process parameters</li> <li>we can update the process parameters whenever we like</li> <li>we can check whether the ARMA process is stable.</li> </ul> <p>Let\u2019s get to it.</p> <pre><code>class ARMA:\n\n    def __init__(self, alpha_0, alpha, beta, sigma):\n        self.alpha_0 = alpha_0\n        self.alpha = alpha\n        self.beta = beta\n        self.sigma = sigma\n        self.arma_dict = {\n            'alpha_0': self.alpha_0,\n            'alpha': self.alpha,\n            'beta': self.beta,\n            'sigma': self.sigma\n        }\n\n    # Methods to update the parameters help in this class\n    def set_alpha_0(self, alpha_0):\n        self.alpha_0 = alpha_0\n        self.arma_dict['alpha_0'] = alpha_0\n\n    def set_alpha(self, alpha):\n        self.alpha = alpha\n        self.arma_dict['alpha'] = alpha\n\n    def set_beta(self, beta):\n        self.beta = beta\n        self.arma_dict['beta'] = beta\n\n    def set_sigma(self, sigma):\n        self.sigma = sigma\n        self.arma_dict['sigma'] = sigma\n\n    # the simulation method\n    def sim_arma(self, T):\n        p = len(self.alpha)\n        q = len(self.beta)\n        alpha = self.alpha\n        beta = self.beta\n\n        y = np.empty(1000+T) # 1000 burn-in draws\n        eps = np.random.normal(0, arma_dict['sigma'], T+1000)\n        y[0:max(p,q)] = eps[0:max(p,q)]  \n        for i in np.arange(max(p,q)+1, T+1000):\n            y[i] = np.vdot(y[i-(p+1):(i-1)], alpha) + np.vdot(eps[i-(q+1):(i-1)], beta)\n\n        return y[-T:]\n\n    # checking for stability\n    def is_stable(self):\n        print(self.alpha)\n        coefs = np.concatenate([[1], - self.alpha])[::-1] # [::-1] reverses an array\n        print(\"-\"*70)\n        print(\"The lag polynomial to check is:\")\n        print(\"1 -\", \" - \".join([f\"{a}*L^{i+1}\" for i, a in enumerate(self.alpha)]))\n        print(\"\\nThe roots are:\")\n        roots = np.roots(coefs)\n        for root in roots:\n            print(f\" {root:.2f}\")\n\n        # Check if all roots have modulus &gt; 1\n        are_roots_outside_unit_circle = all(np.abs(roots) &gt; 1)\n        if are_roots_outside_unit_circle : \n            print(\"\\nThe process is stable.\")\n        else :\n            print(\"\\nThe process is unstable.\")\n        print(\"-\"*70)\n        return are_roots_outside_unit_circle\n</code></pre> <pre><code># initialize myarma object\nmyarma = ARMA(0, np.array([0.3,0.3]), np.array([0.3]), 1)\n# run and plot a little simulation\nplot_time_series(myarma.sim_arma(1000), myarma.arma_dict)\nmyarma.is_stable()\n\n# change the coefficient vector on AR-part \nmyarma.set_alpha(np.array([3, 1]))\nmyarma.is_stable()\n</code></pre> <p></p> <pre><code>[0.3 0.3]\n----------------------------------------------------------------------\nThe lag polynomial to check is:\n1 - 0.3*L^1 - 0.3*L^2\n\nThe roots are:\n -2.39\n 1.39\n\nThe process is stable.\n----------------------------------------------------------------------\n[3 1]\n----------------------------------------------------------------------\nThe lag polynomial to check is:\n1 - 3*L^1 - 1*L^2\n\nThe roots are:\n -3.30\n 0.30\n\nThe process is unstable.\n----------------------------------------------------------------------\n\n\n\n\n\nFalse\n</code></pre>"},{"location":"sem/3-application_stoch_proc/#markov-processes","title":"Markov Processes","text":""},{"location":"sem/3-application_stoch_proc/#introduction-to-discrete-markov-processes","title":"Introduction to Discrete Markov Processes","text":"<p>A Discrete Markov Process (or Markov Chain) is a mathematical model describing a system that transitions between a finite or countable number of states in discrete time steps. The key feature of a Markov process is the Markov Property, which states that the future state depends only on the current state and not on the sequence of events that preceded it.</p>"},{"location":"sem/3-application_stoch_proc/#key-definitions","title":"Key Definitions:","text":"<ul> <li> <p>State Space (S): The set of all possible states the system can be in. It can be finite or countably infinite.</p> </li> <li> <p>Time Parameter: Discrete, often represented as t = 0, 1, 2, \\ldots.</p> </li> <li> <p>Transition Probability:</p> <ul> <li>Denoted as P_{ij}, it represents the probability of transitioning from state i to state j in one time step.</li> <li>Mathematically: P_{ij} = P(X_{t+1} = j \\mid X_t = i), where X_t is the state at time t.</li> <li>Collect these in a matrix, \\Pi</li> <li>The sum of probabilities in each row equals 1: \\sum_{j} P_{ij} = 1 for all i.</li> </ul> </li> <li> <p>Initial Distribution (\\pi^{(0)}):</p> <ul> <li>A probability distribution over the state space at time t = 0.</li> </ul> </li> <li> <p>n-Step Transition Probability:</p> <ul> <li>The probability of transitioning from state i to state j in n steps, denoted as \\Pi_{ij}^{(n)}.</li> <li>Calculated by raising the transition matrix to the n^{th} power: (\\Pi^{(n)})' = (\\Pi')^n.</li> </ul> </li> <li> <p>Stationary Distribution (\\pi):</p> <ul> <li>A probability distribution over states that remains unchanged as the process evolves.</li> <li>Satisfies \\pi = \\Pi' \\pi.</li> <li>Represents the long-term behavior of the Markov process if it exists and is unique.</li> </ul> </li> </ul> <p>Given a distribution \\pi_t, the next period distribution will be \\pi_{t+1} = \\Pi' \\pi_t</p> <pre><code># transition matrix\nPi = np.array([\n    [0.2, 0.4, 0.4],\n    [0.1, 0.5, 0.4],\n    [0.8, 0.1, 0.1]\n])\n\n# current distribution\npi = np.array([0.5, 0.5, 0])\npi = pi[:, np.newaxis]\n\n# next period distribution\nPi.transpose() @ pi\n</code></pre> <pre><code>array([0.15, 0.45, 0.4 ])\n</code></pre> <p>Exercise: Write a function that checks whether a given matrix is a Markov matrix.</p> <ul> <li>do the columns in a given row sum to 1?  </li> <li>are all entries between 0 and 1?</li> <li>is it a square matrix?</li> </ul> <p>Then, write a function which takes a Markov transition matrix and calculates the stationary distribution. (Hint: \\Pi^N converges to a matrix which contains the stationary distribution(s) in its rows.) </p> <pre><code># TBD\n\nM = Pi\nfor i in range(50):\n    M = Pi @ M\nM\n</code></pre> <pre><code>array([[0.35042735, 0.34188034, 0.30769231],\n       [0.35042735, 0.34188034, 0.30769231],\n       [0.35042735, 0.34188034, 0.30769231]])\n</code></pre>"},{"location":"sem/3-application_stoch_proc/#rouwenhorst-method-to-approximate-an-ar1-process-with-a-markov-chain","title":"Rouwenhorst Method to Approximate an AR(1) Process with a Markov Chain","text":""},{"location":"sem/3-application_stoch_proc/#advantages-of-the-rouwenhorst-method","title":"Advantages of the Rouwenhorst Method:","text":"<ul> <li>Flexibility: The Rouwenhorst method is particularly useful for approximating AR(1) processes with high persistence (i.e., when $ \\rho $ is close to 1) because it can accommodate the high persistence and the correct distribution properties of the AR(1) process.</li> <li>Accuracy: It provides a good approximation with relatively few states (even with a small $ n $), making it computationally efficient.</li> </ul> <p>For an arbitrary Markov process mapping to income states and corresponding income levels $ y $, consider the simplest case:</p>  \\begin{align*} \\log y_t &amp;= \\rho \\log y_{t-1} + \\epsilon_t, \\quad \\epsilon_t \\sim N(0, \\alpha^2), \\\\ \\alpha^2 &amp;= \\mathrm{Var}(\\log y_t) (1 - \\rho^2). \\end{align*}  <ul> <li> <p>Note that \\mathrm{Var}(\\log y_t) is the long-run variance as well as the cross sectional variance, which is typically directly estimated. So is \\rho, and we infer \\alpha.  </p> </li> <li> <p>Our goal is to approximate this continuous AR(1) process with n discrete states using the Rouwenhorst Method. This method helps us construct a Markov transition matrix \\Pi that closely matches the properties of the AR(1) process.</p> </li> <li> <p>To approximate the AR(1) process, we represent it with n discrete states. Each state is a sum e_t \\in \\{0,1,..., n-1\\} of n-1 underlying hidden binary state variables. Each binary state has a probability p of staying at its current value and a probability 1-p of switching to a different value.</p> </li> <li> <p>The parameter p is set to match the persistence of the AR(1) process, where p = \\frac{1+\\rho}{2}. The standard deviation of the underlying state e_t is given by \\frac{\\sqrt{n-1}}{2}. To match the cross-sectional standard deviation of log income, we scale (the grid of) e_t by \\frac{\\alpha}{\\sqrt{1 - \\rho^2}} \\frac{2}{\\sqrt{n-1}} = \\sigma_y \\frac{2}{\\sqrt{n-1}}.</p> </li> <li> <p>Finally, the goal is to find the discretized income process corresponding to these states.</p> </li> </ul> <p>The Markov transition matrix \\Pi^n for the states e follows the recursion:</p>  \\tilde{\\Pi}^{n} = p \\begin{bmatrix} \\Pi^{n-1} &amp; \\mathbf{0} \\\\ \\mathbf{0}' &amp; 0 \\end{bmatrix}  + (1-p) \\begin{bmatrix} \\mathbf{0} &amp; \\Pi^{n-1} \\\\ 0 &amp; \\mathbf{0} \\end{bmatrix}  + (1-p) \\begin{bmatrix} \\mathbf{0}' &amp; 0 \\\\ \\Pi^{n-1} &amp; \\mathbf{0} \\end{bmatrix}  + p \\begin{bmatrix} 0 &amp; \\mathbf{0}' \\\\ \\mathbf{0} &amp; \\Pi^{n-1} \\end{bmatrix}  \\tag{6}  <p>The final transition matrix \\Pi^n is equal to \\tilde{\\Pi}^{n} for the first and last rows, and \\tilde{\\Pi}^{n}/2 for all other rows. The base case for the recursion is:</p>  \\Pi^{2} = \\begin{bmatrix} p &amp; 1-p \\\\ 1-p &amp; p \\end{bmatrix}  <p>This procedure can be implemented in a function <code>rouwenhorst(n, rho, sd_log_y)</code> which returns a transition matrix and a grid for \\log y.</p> <p>Let\u2019s get to it:</p> <pre><code># sigma is the sd of the error, e_t\n@jit(nopython=True)\ndef rouwenhorst(n, rho, sd_log_y):\n\n    # the grid    \n    e = np.arange(n) # sd of e on this grid with Pi is sqrt(n-1)/2\n    e = e / ( (n-1)**0.5 /2 ) # now its unit sd\n    e = e * sd_log_y # now it's the sd of the cross section of log_y\n\n    # the transition matrix\n    p = (1+rho)/2\n    Pi = np.array([[p, 1-p], [1-p, p]])\n\n    while Pi.shape[0] &lt; n:\n        Pi_next = np.zeros((1+Pi.shape[0], 1+Pi.shape[1]))\n        Pi_next[0:Pi.shape[0], 0:Pi.shape[1]] += Pi * p\n        Pi_next[0:Pi.shape[0], -Pi.shape[1]:] += Pi * (1-p)\n        Pi_next[-Pi.shape[0]:, -Pi.shape[1]:] += Pi * p\n        Pi_next[-Pi.shape[0]:, 0:Pi.shape[1]] += Pi * (1-p)\n        Pi_next[1:-1, :] /= 2\n        Pi = Pi_next\n\n    return Pi, e\n\n@jit(nopython=True)\ndef stationary_dist(Pi):\n    Pi_stationary = Pi.copy()\n    eps = 1\n    while eps &gt; 10E-12:\n        Pi_old = Pi_stationary.copy()\n        Pi_stationary = Pi_stationary @ Pi_stationary\n        eps = np.max(np.abs(Pi_stationary - Pi_old))\n\n    if np.max(\n            np.abs( \n                np.sum(Pi_stationary - Pi_stationary,axis = 0) / Pi_stationary.shape[0]\n            )\n        ) &lt; 10E-10: # the ugly sum.../ .shape construction is because numpy cant handle np.mean with axis args\n        print(\"the steady state is unique.\")\n\n    return Pi_stationary\n\ndef normalize_y(log_y, pi_ss): # make y have unit mean\n        y = np.exp(log_y)\n        y = y / np.vdot(y, pi_ss)\n        return y\n</code></pre> <pre><code># lets test our code\nPi, log_y = rouwenhorst(20, 0.975, 0.7)\npi_ss = stationary_dist(Pi)[0,:]\ny_grid = normalize_y(log_y, pi_ss)\n\n# plot income and probability distribution\nplt.figure(figsize=(10, 6))\nplt.plot(y_grid, pi_ss, marker='o', linestyle='-', color='b', markersize=5, linewidth=1)\nplt.title('Steady State Income Distribution', fontsize=16)\nplt.xlabel('Income', fontsize=14)\nplt.ylabel('Probability', fontsize=14)\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>the steady state is unique.\n</code></pre> <p></p>"},{"location":"sem/4-optimization/","title":"Introduction to Equation Solving &amp; Optimization in Python","text":"<p>Python provides powerful tools and libraries for solving various types of equations, ranging from simple systems of linear equations to more complex polynomial and non-linear equations. This introduction will guide you through the basics of solving these equations using Python.</p>"},{"location":"sem/4-optimization/#1-solving-systems-of-linear-equations","title":"1. Solving Systems of Linear Equations","text":"<p>A system of linear equations can be represented in matrix form as Ax = b, where:</p> <ul> <li>A is a matrix of coefficients,</li> <li>x is a vector of unknowns,</li> <li>b is a vector of constants.</li> </ul> <p>In Python, you can solve such systems using the <code>numpy</code> library:</p> <pre><code>import numpy as np\n\n# Coefficient matrix A\nA = np.array([[3, 1], [1, 2]])\n\n# Constant vector b\nb = np.array([9, 8])\n\n# Solving for x\nx = np.linalg.solve(A, b)\n\nprint(x)\n</code></pre> <pre><code>[2. 3.]\n</code></pre> <p>Exercises:</p> <ol> <li>Write a function <code>fit</code> taking in a np.array <code>y</code> and a matrix <code>X</code> of conformable dimensions (X\\in \\mathbb{R}^{n\\times k}), such that it calculates the OLS estimator \\hat{\\beta}(X), as well as the residual vector \\epsilon. Do not solve the normal equations with a matrix inverse, but use <code>np.linalg.solve</code>.</li> <li>Write a function <code>robust</code> which calculates the sandwich estimator: </li> </ol> n\\text{Var}(\\hat{\\beta})_{\\text{robust}} = (X'X)^{-1} \\left( \\sum_{i=1}^n X_i' \\hat{\\epsilon}_i^2 X_i \\right) (X'X)^{-1} <ol> <li>Write a <code>class</code> called <code>olsmodel</code> which holds <code>y</code> and <code>X</code>, and which contains methods <code>fit</code>, <code>robust</code> and <code>predict</code></li> </ol> <p>Solution</p> <pre><code>def sim_some_data(size, ncov):\n    X = np.random.normal(size=(size, ncov))\n    beta = np.random.randint(-5,5, size = ncov)\n    X[:, 0] = 1\n    y = X @ beta + np.random.normal(loc=0, scale=ncov, size=size) \n    return y, X, beta \n\ny, X, beta = sim_some_data(100_000, 100)\n\ndef fit(y, X):\n    beta_hat = np.linalg.solve( X.T @ X, X.T @ y )\n    resid = y - X @ beta_hat\n    return beta_hat, resid\n\nbeta_hat, resid = fit(y, X)\n\n# import matplotlib.pyplot as plt\n# plt.scatter(beta, beta_hat)\n# plt.title(\"Coefficient plot\")\n# plt.xlabel(r\"$\\beta$\")\n# plt.ylabel(r\"$\\hat \\beta$\")\n# plt.show()\n</code></pre> <pre><code>from numba import njit\n\n@njit\ndef robust(X, resid):\n    k = X.shape[1]\n    n = X.shape[0]\n    sum = np.zeros((k, k))\n    for i in range(n): # we prefer using a loop over using residuals on a diagonal matrix, so we don't have to instantiate a NxN matrix \n        sum = sum + np.outer(X[i, :],  X[i, :]) * (resid[i]**2)\n\n    XpX_inv = np.linalg.inv(X.T @ X)\n\n    sw = (XpX_inv @ sum @ XpX_inv)\n    return sw\n\n_ = robust(X, resid)\n\n## Or an alternative version using no matrix inversion:\n@njit\ndef robust_linalg(X, resid):\n    k = X.shape[1]\n    n = X.shape[0]\n    sum = np.zeros((k, k))\n    for i in range(n): # we prefer using a loop over using residuals on a diagonal matrix, so we don't have to instantiate a NxN matrix \n        sum = sum + np.outer(X[i, :],  X[i, :]) * (resid[i]**2)\n\n    XpX = X.T @ X\n\n    # steps:\n    # XpX @ sw @ XpX = sum\n    sw_XpX = np.linalg.solve(XpX , sum)\n\n    # steps:\n    # sw @ XpX = sw_XpX\n    # XpX.T @ sw.T = sw_XpX.T\n    sw_T = np.linalg.solve(XpX.T, sw_XpX.T) # transpose of sandwich sw (but sw is diagonal)\n\n    return sw_T\n\n_ = robust_linalg(X, resid) # compile function and throw result away\n</code></pre> <pre><code>%timeit robust(X, resid)\n%timeit robust_linalg(X, resid)\n</code></pre> <pre><code>1.19 s \u00b1 31.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n1.32 s \u00b1 110 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>"},{"location":"sem/4-optimization/#2-finding-roots-of-polynomial-equations","title":"2. Finding Roots of Polynomial Equations","text":"<p>For polynomial equations, Python\u2019s <code>numpy</code> library offers straightforward methods to find roots. For example, to solve a polynomial equation like ax^2 + bx + c = 0.  This code computes the roots of the quadratic equation x^2 - 3x + 2 = 0, which are the values of x that satisfy the equation.</p> <pre><code>coefficients = [1, -3, 2]  # Coefficients of x^2 - 3x + 2\nroots = np.roots(coefficients)\n\nprint(roots)\n</code></pre> <pre><code>[2. 1.]\n</code></pre> <p>Exercise: Write a function <code>IRR</code> to calculate the internal rate of return of a payment stream. The function takes as arguments a stream of future payments x_1,...,x_n and an initial payment C_0, and finds the roots of the equation $$ C_0 = \\sum_i x_i (1+r)^{-i}.$$ It then checks which of the roots \\{r_1,...,r_n\\} are real, and picks among those the internal rate of return. Does the function work well for large n?</p> <pre><code>C_0 = 100  # Initial investment\nx = np.array([20, 26, 40, 55])  # Cash flows, the last payment comes last in this sequence\n\ndef IRR(C_0, x):\n    # Reverse the cash flows array to match the polynomial root finding convention\n    x = np.flip(x)\n\n    # Create the coefficients array for the polynomial equation\n    coefficients = np.concatenate([x, [-C_0]])\n\n    # Find the roots of the polynomial equation\n    roots = np.roots(coefficients)\n\n    # Filter out the complex roots, keep only real roots\n    is_real_solution = np.real(roots) == roots\n    roots = roots[is_real_solution]\n    roots = np.real(roots)\n\n    # Calculate IRR candidates from the real roots\n    IRR_candidates = roots**(-1) - 1\n\n    # Filter out IRR candidates that are greater than -1\n    IRR = IRR_candidates[IRR_candidates &gt; -1]\n\n    # Return the IRR if there is a unique solution, otherwise print a message\n    if IRR.size == 1:\n        return IRR[0]\n    else:\n        print(\"non-unique IRR\")\n\n# Call the IRR function with the initial investment and cash flows\nIRR(C_0, x)\n</code></pre> <pre><code>0.1280272166910017\n</code></pre>"},{"location":"sem/4-optimization/#3-newton-methods-for-general-non-linear-equations","title":"3. Newton Methods for General Non-Linear Equations","text":"<p>We now try to understand Newton\u2019s workhorse optimization routine.</p>"},{"location":"sem/4-optimization/#newtons-method","title":"Newton\u2019s Method","text":""},{"location":"sem/4-optimization/#1-basic-idea","title":"1. Basic Idea","text":"<p>Newton\u2019s method iteratively approximates the root of a function using the following update rule:</p>  x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}  <p>where:</p> <ul> <li>x_n is the current estimate of the root.</li> <li>f'(x_n) is the derivative (or Jacobian, in the multivariate case) of f(x) at x_n.</li> </ul>"},{"location":"sem/4-optimization/#2-algorithm","title":"2. Algorithm","text":"<ol> <li>Start with an initial guess x_0.</li> <li>Compute the function value f(x_n) and its derivative f'(x_n).</li> <li>Update the estimate using the formula x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}.</li> <li>Repeat until the change in x is smaller than a specified tolerance, or the function value f(x_n) is sufficiently close to zero.</li> </ol> <p>Exercise: With pen and paper, prove that Newton\u2019s method converges in one step for any linear equation Ax = b</p> <pre><code>from scipy.optimize import fsolve\n\n# varaible intercept for equation below\ninterc = 5\n\ndef equation(x, interc):\n    return x**3 - x + interc\n\nprint(\"roots found by numpy:\", np.roots([1, 0, -1, + interc]))\n\ninitial_guess = -2\nx_root, fsolve_dict, code, info = fsolve(equation, initial_guess, args=(interc), full_output=True)\nprint(fsolve_dict)\n\nplt.plot(np.linspace(-3, 3, 100),equation(np.linspace(-3, 3, 100), interc))\nplt.axhline(0, color='red', linestyle='--', label='y=0')  # Add horizontal line at y=0\nplt.scatter(x_root, equation(x_root, interc))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot of the Equation with Horizontal Line at y=0')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(\"Note that the fsolve routine gets stuck for initial values in the region (-1, 1).\")\n</code></pre> <pre><code>roots found by numpy: [-1.90416086+0.j          0.95208043+1.31124804j  0.95208043-1.31124804j]\n{'nfev': 7, 'fjac': array([[-1.]]), 'r': array([-9.87749]), 'qtf': array([1.12003029e-09]), 'fvec': array([0.])}\n</code></pre> <p></p> <pre><code>Note that the fsolve routine gets stuck for initial values in the region (-1, 1).\n</code></pre> <p>Let\u2019s write a multivariate Newton solver able to solve equations of type F(x)=0, \\; F: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n. The solver should take as inputs a funtion to solve, <code>f</code>, a vector <code>x0</code> as a starting point, and a tolerance level <code>tol</code> for convergence, as well as a <code>maxiter</code> number of iterations after which it stops the solution process. We proceed in a few steps, in particular, first we need a method to obtain the jacobian, <code>Jf</code> at an arbitrary point:</p> <pre><code># example function I\ndef f(x):\n    return np.array([\n        x[0]**2 - 2 + 4*x[1], \n        - x[1] + x[0]**5\n    ])\nx0 = np.array([1., 1.])\n\n# # example function II\n# def f(x):\n#     return np.array([\n#         x[0]**2, \n#         x[1]**5\n#     ])\n# x0 = np.array([1., 1.])\n\n# # example function III\n# def f(x):\n#     return np.array([\n#         x[0]**2\n#     ])\n# x0 = np.array([1.])\n\nf(x0)\n</code></pre> <pre><code>array([3., 0.])\n</code></pre> <pre><code># run-off-the-mill method to calculate a jacobian numerically\ndef jacobian(f, x0):\n\n    x0 = np.array(x0, dtype='float64') \n\n    fx0 = f(x0)\n    M, N = *fx0.shape, *x0.shape\n    Jf = np.empty((M, N))\n    epsilon = 1E-8\n\n    for i in range(N):\n        x_eps = x0.copy()\n        x_eps[i] += epsilon\n        Jf[i, :] = (f(x_eps) - fx0) / epsilon\n\n    return Jf\n\n# test the function\njacobian(f, x0)\n</code></pre> <pre><code>array([[ 1.99999999,  5.00000008],\n       [ 3.99999998, -0.99999999]])\n</code></pre> <pre><code># newton solver\ndef newton(f, x0, tol = 1E-12, maxiter=1_000):\n\n    x_old = x0\n    x_new = x_old.copy()\n    Jf = jacobian(f, x_old)\n\n    for i in range(maxiter):\n\n        x_old = x_new.copy()\n        f_old = f(x_new)\n        if np.all( np.abs(f_old) &lt; tol ) :\n            print(f\"convergence achieved after {i} iterations\")\n            return x_new, f_old, Jf\n\n        Jf = jacobian(f, x_old)\n        #print(f_old)\n        x_new = x_old - np.linalg.inv(Jf) @ f_old\n\n    print(\"convergence not achieved\")\n    return x_old, f_old, Jf\n</code></pre> <pre><code># it works!\nnewton(f, x0, maxiter=10_000)\n</code></pre> <pre><code>convergence achieved after 124 iterations\n\n\n\n\n\n(array([0.804978  , 0.33800261]),\n array([-8.11350986e-13,  1.27675648e-13]),\n array([[ 1.60995601,  2.09945251],\n        [ 4.        , -1.        ]]))\n</code></pre> <p>Exercise: Secant Method</p> <p>The Secant method is a derivative-free variation of Newton\u2019s method. Instead of using the exact derivative f'(x), it approximates the derivative using two recent points:</p>  x_{n+1} = x_n - f(x_n) \\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}  <ul> <li>Advantages: Does not require computing derivatives, which can be advantageous when the derivative is difficult to calculate.</li> <li>Disadvantages: Typically converges more slowly than Newton\u2019s method.</li> </ul> <p>Write a univariate root solver <code>secant_newton</code> in the spirit of the <code>newton</code> solver we just developed, which uses the secant method. </p>"},{"location":"sem/4-optimization/#4-application-optimal-life-time-consumption","title":"4. Application: Optimal Life-Time Consumption","text":"<p>Let\u2019s use the solver we just wrote to solve a class of simple optimal consumption problems. In T periods, an agent can decide to save or consume, given an initial endowment \\omega and some income y_t, which varies every period.</p>   U = \\sum_{0\\leq t \\leq T-1} \\beta^t \\log(c_t), \\text{  s.t.  } a_{t+1} = y_t + a_t*(1+r) - c_t,\\; a_{-1} = \\omega,\\; a_{T} \\geq 0   <p>Giving us FOCs: $$ \\begin{equation}     f_0(\\omega, a_0, a_1) = 0,  \\end{equation} $$</p>  \\begin{equation*}     f_t(a_{t-1}, a_t, a_{t+1}) \\equiv \\beta (1+r) ( y_{t-1} + a_{t-1}(1+r) - a_t ) - ( y_t + a_t(1+r) -a_{t+1}) = 0 ,\\; \\forall 1 \\leq t \\leq T-2  \\end{equation*}   \\begin{equation*}     f_{T-2}(a_{T-2}, a_{T-1}, 0) =0  \\end{equation*}  <p>to solve simultaneously by choosing a_0, ..., a_{T-1}. We could do this in a recursive way, but lets attack the FOCs directly.</p> <pre><code>beta = 0.98\nr = 1/beta - 1\nomega = 5\n\n# y has T elements \ny = np.full(10, 1)\n\n# a_choice has T elements \na_choice = np.full(10, 0)\n\ndef F(beta, r, omega, y, a_choice):\n\n    a = np.zeros((1+len(y))) # accommodate initial and terminal condition\n    a[0:-1] = a_choice\n\n    F = np.zeros(len(y))\n    F[0] = beta*(1+r) * ( 0 + omega * (1+r) - a[0] ) - ( y[0] + a[0]*(1+r) - a[1] )\n\n    for t in range(1, len(F)):\n        F[t] = beta*(1+r) * ( y[t-1] + a[t-1] * (1+r) - a[t] ) - ( y[t] + a[t]*(1+r) - a[t+1] )\n\n    return F\n</code></pre> <pre><code>F(beta, r, omega, y, a_choice)\n</code></pre> <pre><code>array([4.10204082, 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ])\n</code></pre> <pre><code># try the function\nprint(\"F =\", f\"{F(beta, r, omega, y, a_choice)}\")\n\n# Does it jacobe? Yes, it does!\nJ = jacobian(lambda a_choice : F(beta, r, omega, y, a_choice), a_choice)\n\nassert np.linalg.det(J) != 0 # check that the jacobian is not ill conditioned\n</code></pre> <pre><code>F = [4.10204082 0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ]\n</code></pre> <p>Let\u2019s try whether this works out, and whether our solver can find a sequence of assets a = (a_0, ..., a_{T-1}) to solve the first order conditions:</p> <pre><code>a_choices, F_values, _ = newton(lambda a_choice : F(beta, r, omega, y, a_choice), a_choice, maxiter=10_000)\n</code></pre> <pre><code>convergence achieved after 10 iterations\n</code></pre> <p>Indeed, we have solved the consumption-savings problem with the output of our Newton-solver. Let\u2019s plot these results in a figure.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Plot the asset path\nplt.figure(figsize=(5, 3))\nplt.plot(a_choices, marker='o', linestyle='-', color='b', markersize=5, linewidth=1)\nplt.title('Asset Path in Consumption-Savings Problem', fontsize=16)\nplt.xlabel('Time Period', fontsize=14)\nplt.ylabel('Assets', fontsize=14)\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Looks great!</p>"},{"location":"sem/4-optimization/#a-consumption-savings-class","title":"A Consumption-Savings Class","text":"<p>Now let\u2019s go all in and package the problem nicely. We write a class <code>ConSavProb</code> which takes as inputs <code>beta, r, y, omega</code> and an initial guess for <code>a</code>. It has a <code>solve</code> method, which solves for the optimal asset and consumption path.</p> <p>Packaging the consumption-savings problem into a class, in this case <code>ConSavProb</code>, offers several benefits:</p> <ol> <li> <p>Modularity: By encapsulating the problem within a class, we can organize related variables and functions together. This improves code organization and makes it easier to understand and maintain. It also allows us to reuse the class in different parts of our code or even in other projects.</p> </li> <li> <p>Abstraction: Instead of exposing all the inner workings of the consumption-savings problem, we can provide a clean interface through class methods. </p> </li> <li> <p>Encapsulation: Classes allow us to encapsulate data and methods together. This means that the variables and functions related to the consumption-savings problem are contained within the class, reducing the chances of naming conflicts with other parts of the codebase. It also provides a clear boundary for the problem, making it easier to reason about and test.</p> </li> <li> <p>Code Reusability: Once we have defined the <code>ConSavProb</code> class, we can create multiple instances of it with different input parameters.</p> </li> <li> <p>Readability: Using a class can improve the readability of the code. </p> </li> </ol> <p>Overall, using a class to package the consumption-savings problem provides a clean, modular, and reusable solution that enhances code organization, abstraction, and readability.</p> <pre><code>class ConSavProb:\n    \"\"\"\n    A class representing a consumption-savings problem.\n\n    Attributes:\n        beta (float): The discount factor.\n        r (float): The interest rate.\n        y (float): The income.\n        omega (float): The initial endowment.\n        asset_path (numpy.ndarray): The path of assets over time.\n        a_guess (float): The initial guess for assets.\n        euler_error (float): The Euler equation error.\n        solved (bool): Indicates whether the problem has been solved.\n\n    Methods:\n        update_parameters: Update the parameters of the problem.\n        solve_asset_path: Solve the consumption-savings problem and compute the asset path.\n        plot_asset_path: Plot the asset path.\n\n    \"\"\"\n\n    def __init__(self, beta, r, y, omega):\n        \"\"\"\n        Initialize a ConSavProb object.\n\n        Args:\n            beta (float): The discount factor.\n            r (float): The interest rate.\n            y (float): The income.\n            omega (float): The probability of receiving income.\n\n        \"\"\"\n        self.beta = beta\n        self.r = r\n        self.y = y\n        self.omega = omega\n        self.asset_path = None\n        self.euler_error = None\n        self.solved = False\n\n    def update_parameters(self, beta=None, r=None, y=None, omega=None):\n        \"\"\"\n        Update the parameters of the problem.\n\n        Args:\n            beta (float, optional): The discount factor.\n            r (float, optional): The interest rate.\n            y (float, optional): The income.\n            omega (float, optional): The probability of receiving income.\n\n        \"\"\"\n        if beta is not None:\n            self.beta = beta\n        if r is not None:\n            self.r = r\n        if y is not None:\n            self.y = y\n        if omega is not None:\n            self.omega = omega\n\n    def solve_asset_path(self, a_guess=None):\n        \"\"\"\n        Solve the consumption-savings problem and compute the asset path.\n\n        Args:\n            a_guess (float): The initial guess for assets.\n\n        \"\"\"\n        if a_guess is None:\n            a_guess = np.zeros(len(self.y))\n\n        # solve\n        self.asset_path, self.euler_error, _ = newton(self.FOC, a_guess, maxiter=10_000)\n        self.solved = True\n\n    def FOC(self, a_choice):\n        beta, r, omega, y = self.beta, self.r, self.omega, self.y  # unpack the parameters\n        a = np.zeros((1+len(y))) # accommodate initial and terminal condition\n        a[0:-1] = a_choice\n\n        F = np.zeros(len(y))\n        F[0] = beta*(1+r) * ( 0 + omega * (1+r) - a[0] ) - ( y[0] + a[0]*(1+r) - a[1] )\n\n        for t in range(1, len(F)):\n            F[t] = beta*(1+r) * ( y[t-1] + a[t-1] * (1+r) - a[t] ) - ( y[t] + a[t]*(1+r) - a[t+1] )\n\n        return F\n\n    def plot_asset_path(self, figsize=(10, 6)):\n        \"\"\"\n        Plot the asset path.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (10, 6).\n\n        \"\"\"\n        if self.solved == True:\n            # Plot the asset path\n            plt.figure(figsize=figsize)\n            plt.plot(np.concatenate([self.asset_path, [0.]]), marker='o', linestyle='-', color='b', markersize=5,\n                     linewidth=1)\n            plt.title('Asset Path in Consumption-Savings Problem')\n            plt.xlabel('Time Period')\n            plt.ylabel('Assets')\n            plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n            plt.axhline(0, color='black', linewidth=0.5)\n            plt.axvline(0, color='black', linewidth=0.5)\n            plt.tight_layout()\n            plt.show()\n</code></pre> <pre><code>model = ConSavProb(beta, r, y, omega)\nmodel.solve_asset_path(a_guess = np.full(10, 0))\n\nmodel.plot_asset_path(figsize=(7, 4))\n</code></pre> <pre><code>convergence achieved after 10 iterations\n</code></pre> <p></p> <pre><code>model.update_parameters(beta = 0.85)\nmodel.solve_asset_path(model.asset_path)\nmodel.plot_asset_path(figsize=(7, 4))\n</code></pre> <pre><code>convergence achieved after 30 iterations\n</code></pre> <p></p> <p>What, if income were decreasing from 6 to 1 over time, and we had T=20 periods instead?</p> <pre><code>model.update_parameters(y = np.linspace(6,0, 25), beta = 0.85)\nmodel.solve_asset_path()\nmodel.plot_asset_path(figsize=(7, 4))\n</code></pre> <pre><code>convergence achieved after 149 iterations\n</code></pre> <p></p>"},{"location":"sem/4-optimization/#3-the-scipy-library-for-solving-and-optimization","title":"3. The <code>scipy</code> Library for Solving and Optimization","text":"<p>For more complex equations, including non-linear systems, Python\u2019s <code>scipy</code> library provides powerful tools. </p>"},{"location":"sem/4-optimization/#1-basic-usage-of-scipyoptimize","title":"1. Basic Usage of <code>scipy.optimize</code>","text":""},{"location":"sem/4-optimization/#example-finding-the-minimum-of-a-function","title":"Example: Finding the Minimum of a Function","text":"<pre><code>import numpy as np\nfrom scipy.optimize import minimize\n\n# Define the function to minimize\ndef f(x):\n    return x**2 + 5*np.sin(x)\n\n# Initial guess\nx0 = 2.0\n\n# Perform the minimization\nresult = minimize(f, x0)\n\nprint(\"Minimum value:\", result.fun)\nprint(\"At x =\", result.x)\n</code></pre> <pre><code>Minimum value: -3.2463942726915187\nAt x = [-1.11051058]\n</code></pre>"},{"location":"sem/4-optimization/#2-solving-a-system-of-linear-equations","title":"2. Solving a System of Linear Equations","text":""},{"location":"sem/4-optimization/#example-using-scipylinalgsolve","title":"Example: Using <code>scipy.linalg.solve</code>","text":"<pre><code>import numpy as np\nfrom scipy.linalg import solve\n\n# Coefficient matrix\nA = np.array([[3, 2], [1, 2]])\n\n# Right-hand side vector\nb = np.array([2, 0])\n\n# Solve the system\nx = solve(A, b)\n\nprint(\"Solution:\", x)\n</code></pre> <pre><code>Solution: [ 1.  -0.5]\n</code></pre>"},{"location":"sem/4-optimization/#3-integration-using-scipyintegrate","title":"3. Integration using <code>scipy.integrate</code>","text":""},{"location":"sem/4-optimization/#example-numerical-integration-with-quad","title":"Example: Numerical Integration with <code>quad</code>","text":"<pre><code>from scipy.integrate import quad\n\n# Define the function to integrate\ndef f(x):\n    return np.exp(-x**2)\n\n# Perform the integration\nresult, error = quad(f, 0, 1)\n\nprint(\"Integral result:\", result)\nprint(\"Estimated error:\", error)\n</code></pre> <pre><code>Integral result: 0.7468241328124271\nEstimated error: 8.291413475940725e-15\n</code></pre>"},{"location":"sem/4-optimization/#4-interpolation-using-scipyinterpolate","title":"4. Interpolation using <code>scipy.interpolate</code>","text":""},{"location":"sem/4-optimization/#example-1d-interpolation-with-interp1d","title":"Example: 1D Interpolation with <code>interp1d</code>","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\n# Sample data points\nx = np.linspace(0, 10, 10)\ny = np.sin(x)\n\n# Create the interpolating function\nf = interp1d(x, y, kind='cubic')\n\n# Interpolated values\nx_new = np.linspace(0, 10, 100)\ny_new = f(x_new)\n\n# Plot the results\nplt.plot(x, y, 'o', label='data points')\nplt.plot(x_new, y_new, '-', label='cubic interpolation')\nplt.legend()\nplt.show()\n</code></pre> <p>These examples should give you a good starting point for using <code>scipy</code> in various scientific and technical computing tasks.</p>"},{"location":"sem/5-endogenous_grid_method/","title":"Endogeneous Grid Method","text":"<p>In this notebook we discuss how to solve the household problem a la Aiyagari\u2013this is the main building block of most HANK models\u2013with fast, optimized methods. The main reference for this notebook is Matt Rognlie\u2019s code on which you can find in his repository (2024).</p>"},{"location":"sem/5-endogenous_grid_method/#1-incomplete-markets-model","title":"1. Incomplete Markets Model","text":"<p>The standard incomplete markets model features the following timing:</p> <ul> <li>begin of period with asset returns (1+r)a and stochastic income y.</li> <li>allocate to assets tomorrow a' and consumption today c</li> <li>all subject to a borrowing constraint a\\geq \\underline a and no ability to insure against idiosyncratic shocks: </li> </ul> <p>$$    \\begin{equation}     V(y, a) = \\max_{c, a\u2019} u( c ) + \\beta \\mathbb E[V(e\u2019, a\u2019) | y]    \\end{equation}   $$ </p> <p>$$    \\begin{equation}     s.t.\\; a\u2019 = (1+r)a + y - c    \\end{equation}   $$ </p> <p>$$    \\begin{equation}     a\u2019 \\geq \\underline a    \\end{equation}   $$</p> <p>We model the income process as an AR(1), so    $$ y_t = \\rho y_{t-1} + e_t.$$   Since we would like to solve the whole model on a finite grid \\mathcal{Y} \\times \\mathcal{A} with indices (i_y, i_a), we need to discretize the income and the asset space. We already have functions to do this, applying the double-exponential grid space for assets, and the Rouwenhorst AR(1) process. </p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# some useful plot defaults$\nplt.rcParams.update({'font.size' : 10, 'lines.linewidth' : 1.5, 'figure.figsize' : (4,3)})\n</code></pre> <p>Exercise: Write and import a class called <code>Grids</code> which if supplied the correct parameters outputs asset and income grids, as well as transition matrix and steady state for income.</p> <p>Solution: A script containing this and other classes (to be defined later in this document) can be found on my github. A non-maintained version is at the bottom of this notebook.</p> <pre><code>from utils_simm import Grid\n</code></pre> <pre><code>Grids = Grid(n_y = 10, rho = 0.975,  sd_log_y = 0.7, n_a = 100, min_a = 0, max_a = 10_000)\n</code></pre> <pre><code>the steady state is unique.\n</code></pre>"},{"location":"sem/5-endogenous_grid_method/#2-backward-iteration-to-obtain-policy-functions","title":"2. Backward Iteration to Obtain Policy Functions","text":"<p>It is more efficient and accurate to use envelope condition and FOC to iterate on marginal value function V_a instead of finding the value function itself. We do backwards iteration in time. Let a_t'(y_t, a_t) be the policy function for next-period asset holdings. Assume that u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}, where \\sigma is the elasiticity of intertemporal substitution (eis).</p> <p>For backward iteration, we use the Envelope condition, $$ V_{a,t}(e, a) = (1+r_t)u\u2019(c_t(e, a))$$</p> <p>and the First-Order Condition (inequality is slack if borrowing limit binds) </p> u'(c_t(y, a)) \\geq \\beta \\mathbb E [V_{a, t+1}(y', a_t'(y, a))]|e].  <p>repeatedly:</p> <p>[Algorithm: Backward iteration in time] </p> <p>Start at t = T, initialize V_{a, T} = 1. For t&lt; T:</p> <ol> <li>Use V_{a, t+1} to calculate RHS of FOC</li> <li>Solve for today\u2019s policies a_t'(y, a), c_t(y, a)</li> <li>Using envelope condition, obtain marginal value function of today V_{a, t}</li> </ol> <p>We go through the steps in detail.</p> <p>Step 1 - Initialize, Set up Grids and Parameters</p> <pre><code>model_params = {\n    'beta' : 0.95,\n    'r' : 0.03,\n    'eis' : 1.5,\n    'rho' : 0.975,\n    'sd_log_y' : 0.7\n}\n\nGrids = Grid(n_y = 15, rho = model_params['rho'],  sd_log_y = model_params['sd_log_y'], n_a = 100, min_a = 0, max_a = 10_000)\n</code></pre> <pre><code>the steady state is unique.\n</code></pre> <pre><code># initial Va\nVa = np.ones((Grids.n_y, Grids.n_a))\n\nW = model_params['beta'] * Grids.Pi @ Va\n</code></pre> <p>Step 2 - Obtain a^\\prime_{t}, W_{t} We now write policy functions, marginal value function and associated objects as functions on the grid indices i_y and i_a whenever useful. We would like to solve for consumption policy using the FOC (assuming the constraint is slack) $$ u\u2019(c_t) = W_{a,t}(y, a\u2019) \\Rightarrow c_t = \\left( W_{a,t}(y, a\u2019) \\right)^{-1/\\sigma}$$ for each possible future asset level a'. Then we use the envelope condition and can obtain V_t(y, a), where a solves a' = (1+r)a + y - c_t(y, a').</p> <p>Problem: The implied a may not lie on the grid. Generally, we want V_{t, a}(y, a) for fixed values of a on the grid, not V_{t, a}(y, a(a')), where a(a') depends on the values of a'\\in \\mathcal{A}.</p> <p>Simple solution: For each fixed level of y\u2026</p> <ul> <li>Obtain the points c_t(y, a), store in <code>c_endog[i_y, :]</code> (these are also called \u2018endogeneous grid points\u2019)</li> <li>Compute (1+r)^{-1} (a' + c_t(y, a') - y) = a, and save the a vector as <code>a_endog[i_y,:]</code>. We now have essentially a function a(y, a')</li> <li>To invert for the policy function, a'(y, a), use linear interpolation evaluated at \\mathcal{A}: <code>np.interpolate(grid_a, a_endog[i_y,:], grid_a)</code></li> <li>Enforce the borrowing constraint by <code>a_prime = np.maximum(a_prime, Grids.grid_a[0])</code></li> </ul> <p>There is a slightly more efficient version of this algorithm using cash-on-hand. See Rognlie (2024).</p> <pre><code>c_endog = W ** (-1/model_params['eis'])\na_endog = (1 + model_params['r'])**(-1) * (Grids.grid_a[np.newaxis, :] - Grids.grid_y[:, np.newaxis] + c_endog)\na_prime = np.empty((Grids.n_y, Grids.n_a))\n\nfor i_y in range(Grids.n_y):\n    a_prime[i_y, :] = np.interp(Grids.grid_a, a_endog[i_y, :], Grids.grid_a)\n\n#a_prime = np.maximum(a_prime, Grids.grid_a[0]) # enforce borrowing constraint. actually not needed because interp already does this\n\n# obtain the consumption policy\nc = (1 + model_params['r']) * (Grids.grid_a[np.newaxis, :] + Grids.grid_y[:, np.newaxis]) - a_prime\n</code></pre> <p>We can look at the policy function after one iteration:</p> <pre><code># plot policy function\nfig, ax = plt.subplots()\nfor i_y in range(Grids.n_y):\n    ax.plot(Grids.grid_a[0:40], a_prime[i_y, 0:40], label = f'y = {Grids.grid_y[i_y]:.2f}')\nax.set(xlabel = r'$a$', ylabel = r'$a^{\\prime}$', title = 'Policy function after 1 iteration')\nplt.show()\n</code></pre> <p></p> <p>Step 3 - Obtain V_{t, a}</p> <pre><code>V_a = (1 + model_params['r']) * c **(-1/model_params['eis']) # update value function derivative\n</code></pre> <p>Combine Steps into single Backward Iteration</p> <p>Now we write a function which takes as input a marginal value function, income and asset grids, preference parameters and transition matrices to compute a single backward iteration and output objects <code>V_a, a, c</code>.</p> <pre><code>def backward_iteration(V_a, beta, eis, r, grid_a, grid_y, Pi):\n\n    n_y, n_a = grid_y.size, grid_a.size\n    W = beta * Pi @ V_a\n\n    c_endog = W ** (-1/eis)\n    a_endog = (1 + r)**(-1) * (grid_a[np.newaxis, :] - grid_y[:, np.newaxis] + c_endog)\n    a_prime = np.empty((n_y, n_a))\n\n    for i_y in range(Grids.n_y):\n        a_prime[i_y, :] = np.interp(grid_a, a_endog[i_y, :], grid_a)\n\n    #a_prime = np.maximum(a_prime, Grids.grid_a[0]) # enforce borrowing constraint. actually not needed because interp already does this\n\n    # obtain the consumption policy\n    c = (1 + r) * (grid_a[np.newaxis, :] + grid_y[:, np.newaxis]) - a_prime\n\n    return c, a_prime\n\n# test the function\nc, a_prime = backward_iteration(V_a, model_params['beta'], model_params['eis'], model_params['r'], Grids.grid_a, Grids.grid_y, Grids.Pi)\n</code></pre> <p>And we can solve the entire household problem for steady-state policies.</p> <pre><code>grid_params = {\n    'n_y' : 7,\n    'n_a' : 500,\n    'min_a' : 0,\n    'max_a' : 10_000\n}\n\nmodel_params = {\n    'beta' : 1-0.08/4, # quarterly discount factor\n    'r' : 0.01/4, # quarterly interest rate\n    'eis' : 1,\n    'rho' : 0.975,\n    'sd_log_y' : 0.7\n}\n\nclass SteadyStateHH:\n\n    def __init__(self, model_params, grid_params, tol = 1e-6, max_iter = 1_000):\n        self.model_params = model_params\n        self.grid_params = grid_params\n        self.Grids = Grid(n_y = grid_params['n_y'], rho = model_params['rho'],  sd_log_y = model_params['sd_log_y'], n_a = grid_params['n_a'], min_a = grid_params['min_a'], max_a = grid_params['max_a'])\n        self.tol = tol\n        self.max_iter = max_iter\n        self.c = None\n        self.a_prime = None\n        self.V_a = None\n\n    def backward_iteration(self, V_a):\n\n        W = self.model_params['beta'] * self.Grids.Pi @ V_a\n\n        c_endog = W ** (-1/self.model_params['eis'])\n        a_endog = (1 + self.model_params['r'])**(-1) * (self.Grids.grid_a[np.newaxis, :] - self.Grids.grid_y[:, np.newaxis] + c_endog)\n        a_prime = np.empty((self.grid_params['n_y'], self.grid_params['n_a']))\n\n        for i_y in range(self.grid_params['n_y']):\n            a_prime[i_y, :] = np.interp(self.Grids.grid_a, a_endog[i_y, :], self.Grids.grid_a)\n\n        #a_prime = np.maximum(a_prime, self.Grids.grid_a[0]) \n\n        # obtain the consumption policy\n        c = (1 + self.model_params['r']) * (self.Grids.grid_a[np.newaxis, :] + self.Grids.grid_y[:, np.newaxis]) - a_prime\n\n        return c, a_prime\n\n    def solve_ss(self):\n\n        # initialize value function derivative with guess\n        if self.V_a is None:\n            V_a = np.ones((self.grid_params['n_y'], self.grid_params['n_a']))\n        else:\n            V_a = self.V_a\n\n        for i in range(self.max_iter):\n            c, a_prime = self.backward_iteration(V_a)\n            V_a_new = (1 + self.model_params['r']) * c **(-1/self.model_params['eis'])\n\n            if np.max(np.abs(V_a_new - V_a)) &lt; self.tol:\n                break\n\n            V_a = V_a_new\n\n        self.c = c\n        self.a_prime = a_prime\n        self.V_a = V_a\n\n        return c, a_prime\n\n    def plot_policy(self, bound_grid = 0.4):\n        \"\"\" \n        Plot the policy function for the first 4 income states\n        bound_grid: float, fraction of the grid to plot\n        \"\"\"\n        rng_asset_grid = int(grid_params['n_a']*bound_grid)\n        fig, ax = plt.subplots()\n        for i_y, y in enumerate(self.Grids.grid_y[0:4]):\n            ax.plot(self.Grids.grid_a[0:rng_asset_grid], self.c[i_y, 0:rng_asset_grid], label = f'y = {y:.2f}')\n        ax.set(xlabel = r'$a$', ylabel = r'$c(y,a)$', title = 'Steady State Policy Function')\n        plt.legend(fontsize = 'small')\n        plt.show()\n\nss = SteadyStateHH(model_params, grid_params)\nss.solve_ss()\nss.c, ss.a_prime\nss.plot_policy(0.2)\n</code></pre> <pre><code>the steady state is unique.\n</code></pre> <p></p> <p>Exercise: Refactor the code in order to use <code>@njit</code>.</p> <pre><code>from numba import njit\n\n@njit\ndef backward_iteration(V_a, beta, eis, r, grid_a, grid_y, Pi):\n    W = beta * Pi @ V_a\n\n    c_endog = W ** (-1/eis)\n    a_endog = (1 + r)**(-1) * (grid_a[np.newaxis, :] - grid_y[:, np.newaxis] + c_endog)\n    a_prime = np.empty((grid_y.shape[0], grid_a.shape[0]))\n\n    for i_y in range(grid_y.shape[0]):\n        a_prime[i_y, :] = np.interp(grid_a, a_endog[i_y, :], grid_a)\n\n    c = (1 + r) * (grid_a[np.newaxis, :] + grid_y[:, np.newaxis]) - a_prime\n\n    return c, a_prime\n\nclass SteadyStateHH:\n\n    def __init__(self, model_params, grid_params, tol = 1e-6, max_iter = 1_000):\n        self.model_params = model_params\n        self.grid_params = grid_params\n        self.Grids = Grid(n_y = grid_params['n_y'], rho = model_params['rho'],  sd_log_y = model_params['sd_log_y'], n_a = grid_params['n_a'], min_a = grid_params['min_a'], max_a = grid_params['max_a'])\n        self.tol = tol\n        self.max_iter = max_iter\n        self.c = None\n        self.a_prime = None\n        self.V_a = None\n\n    # adding the model_params as an argument allows solving for different parameterizations\n    def solve_ss(self, model_params):\n\n        # update grid if necessary\n        if (self.model_params['rho'], self.model_params['sd_log_y']) != (model_params['rho'], model_params['sd_log_y']):\n            self.Grids = Grid(n_y = self.grid_params['n_y'], rho = model_params['rho'],  sd_log_y = model_params['sd_log_y'], n_a = self.grid_params['n_a'], min_a = self.grid_params['min_a'], max_a = self.grid_params['max_a'])\n        # update model_params if necessary\n        if self.model_params != model_params:\n            self.model_params = model_params\n\n        # initialize value function derivative with guess\n        if self.V_a is None:\n            V_a = np.ones((self.grid_params['n_y'], self.grid_params['n_a']))\n        else:\n            V_a = self.V_a\n\n        for i in range(self.max_iter):\n            c, a_prime = backward_iteration(V_a, model_params['beta'], model_params['eis'], model_params['r'], self.Grids.grid_a, self.Grids.grid_y, self.Grids.Pi)\n            V_a_new = (1 + model_params['r']) * c **(-1/model_params['eis'])\n\n            if np.max(np.abs(V_a_new - V_a)) &lt; self.tol:\n                break\n\n            V_a = V_a_new\n\n        self.c = c\n        self.a_prime = a_prime\n        self.V_a = V_a\n\n        return c, a_prime\n\n    def plot_policy(self, bound_grid = 0.4):\n        \"\"\" \n        Plot the policy function for the first 4 income states\n        bound_grid: float, fraction of the grid to plot\n        \"\"\"\n        rng_asset_grid = int(grid_params['n_a']*bound_grid)\n        fig, ax = plt.subplots()\n        for i_y, y in enumerate(self.Grids.grid_y[0:4]):\n            ax.plot(self.Grids.grid_a[0:rng_asset_grid], self.c[i_y, 0:rng_asset_grid], label = f'y = {y:.2f}')\n        ax.set(xlabel = r'$a$', ylabel = r'$c(y,a)$', title = 'Steady State Policy Function')\n        plt.legend(fontsize = 'small')\n        plt.show()\n</code></pre> <pre><code>ss = SteadyStateHH(model_params, grid_params)\nss.solve_ss(model_params=ss.model_params)\nss.c, ss.a_prime\nss.plot_policy(0.2)\n</code></pre> <pre><code>the steady state is unique.\n</code></pre> <p></p> <pre><code>%%timeit \nss = SteadyStateHH(model_params, grid_params)\nss.solve_ss(model_params=ss.model_params)\n</code></pre> <pre><code>ss.solve_ss(ss.model_params)\n</code></pre> <pre><code>(array([[1.41722822e-01, 1.46412415e-01, 1.50292317e-01, ...,\n         1.86182818e+02, 1.94981292e+02, 2.04215800e+02],\n        [2.50991933e-01, 2.55681526e-01, 2.60415096e-01, ...,\n         1.86403939e+02, 1.95202357e+02, 2.04436700e+02],\n        [4.44508157e-01, 4.49197750e-01, 4.53931320e-01, ...,\n         1.86684758e+02, 1.95483026e+02, 2.04717044e+02],\n        ...,\n        [1.34562866e+00, 1.34617631e+00, 1.34672238e+00, ...,\n         1.87539703e+02, 1.96337168e+02, 2.05569687e+02],\n        [2.11335992e+00, 2.11362938e+00, 2.11390126e+00, ...,\n         1.88211194e+02, 1.97007799e+02, 2.06238802e+02],\n        [3.17854812e+00, 3.17874042e+00, 3.17893450e+00, ...,\n         1.89156862e+02, 1.97952041e+02, 2.07180594e+02]]),\n array([[0.00000000e+00, 0.00000000e+00, 8.53668052e-04, ...,\n         8.93360537e+03, 9.36572181e+03, 9.82092592e+03],\n        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n         8.93349352e+03, 9.36561001e+03, 9.82081429e+03],\n        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n         8.93340622e+03, 9.36552286e+03, 9.82072746e+03],\n        ...,\n        [4.85540026e-02, 5.26959516e-02, 5.68834457e-02, ...,\n         8.93350095e+03, 9.36561839e+03, 9.82082450e+03],\n        [3.55745601e-01, 3.60165727e-01, 3.64627417e-01, ...,\n         8.93390438e+03, 9.36602268e+03, 9.82123030e+03],\n        [1.19425196e+00, 1.19874925e+00, 1.20328874e+00, ...,\n         8.93486241e+03, 9.36698214e+03, 9.82219221e+03]]))\n</code></pre>"},{"location":"sem/5-endogenous_grid_method/#3-forward-iteration-to-obtain-distribution","title":"3. Forward Iteration to Obtain Distribution","text":"<p>With steady-state policies at hand, we can compute the distribution of next-period asset holdings, D_{t+1}(y, a) given the current distribution D_t. Assume for a moment that a' maps assets on the grid \\mathcal A. Then </p> D_{t+1}(y, a) = \\sum_{\\bar y \\in \\mathcal Y} \\sum_{\\bar a \\in \\mathcal A} D_{t}(\\bar y, \\bar a) \\mathbb P(\\bar y | y) \\mathbb I(a'(\\bar y, \\bar a) = a)  <p>Problem: a'(e, a) does not map into \\mathcal A. Possible solution: Lotteries.</p> <p>Basic idea is that if a'(e, a) = a' \\in [a_i, a_{i+1}] for two consecutive grid points a_i, a_{i+1}, then a fraction q(y, a) = \\frac{a_{i+1} - a'}{a_{i+1} - a_{i}} of the population lands on the lower gridpoint.</p> <p>Let a^+ be the next highest grid point on the asset grid, and a^- the next lowest. </p> \\begin{align} D_{t+1}(y, a) &amp;= \\sum_{\\bar y \\in \\mathcal Y} \\sum_{\\bar a \\in \\mathcal A} D_{t}(\\bar y, \\bar a) \\mathbb P(\\bar y | y) \\underbrace{\\left[ \\mathbb I(a'(\\bar y, \\bar a)^+ = a) \\frac{ a' - a^-}{a^+ - a^-} + \\mathbb I(a'(\\bar y, \\bar a)^- = a) \\frac{ a^+ - a'}{a^+ - a^-} \\right]}_{\\equiv \\mu( \\bar y, \\bar a, a)} \\\\ &amp;= \\sum_{\\bar y \\in \\mathcal Y} \\sum_{\\bar a \\in \\mathcal A} D_{t}(\\bar y, \\bar a) \\mu( \\bar y, \\bar a, a) \\mathbb P(\\bar y | y) \\\\ &amp;= \\sum_{\\bar y \\in \\mathcal Y}  \\mathbb P(\\bar y | y) \\sum_{\\bar a \\in \\mathcal A} D_{t}(\\bar y, \\bar a) \\mu( \\bar y, \\bar a, a)  \\end{align} <p>We can pre-compute the lottery array \\mu to handle the forward iteration. Still, this approach is very inefficient.</p> <p>Exercise: Explain what is so inefficient about our approach?</p> <p>Exercise: Write a function to compute <code>mu</code></p> <pre><code># Inefficient way to compute lotteries\n@njit\ndef get_mu(policy, grid_a, grid_y):\n\n    assert (policy - grid_a.max()).max() &lt;= 0 and (policy - grid_a.min()).min() &gt;= 0 # make sure policy is within bounds of grid_a\n\n    n_a = np.shape(grid_a)[0]\n    n_y = np.shape(grid_y)[0]\n    mu = np.zeros((n_y, n_a, n_a), dtype=np.float64)\n    for index_a in range(len(grid_a)):\n        for index_a_bar in range(len(grid_a)):\n            for index_y_bar in range(len(grid_y)):\n\n                if (grid_a[index_a] &gt;= policy[index_y_bar, index_a_bar]) and (grid_a[index_a - 1] &lt;= policy[index_y_bar, index_a_bar]) :\n                    p_plus = (policy[index_y_bar, index_a_bar] - grid_a[index_a - 1]) / (grid_a[index_a] - grid_a[index_a-1])\n                    mu[index_y_bar, index_a_bar, index_a]   += p_plus \n                    continue\n\n                if (grid_a[index_a] &lt;= policy[index_y_bar, index_a_bar]) and (grid_a[index_a + 1] &gt;= policy[index_y_bar, index_a_bar]) :\n                    p_minus = (grid_a[index_a + 1] - policy[index_y_bar, index_a_bar]) / (grid_a[index_a+1] - grid_a[index_a])\n                    mu[index_y_bar, index_a_bar, index_a]   += p_minus \n                    continue\n\n    return mu\n\nmu = get_mu(ss.a_prime, ss.Grids.grid_a, ss.Grids.grid_y)\n\n%timeit get_mu(ss.a_prime, ss.Grids.grid_a, ss.Grids.grid_y)\n</code></pre> <pre><code>2.43 ms \u00b1 148 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre> <p>Instead, consider where the mass of D_t(y, a) is sent to. For y fixed, D_t(y, a) \\dfrac{a'(y, a) - a'(y, a)^-}{a'(y, a)^+ - a'(y, a)^-} = D_t(y, a) q(y, a) is sent to a'(y, a)^+ and D_t(y, a) (1-q(y, a)) is sent to a'(y, a)^-. Proceeding over all (y, a), we obtain \\tilde D_t(\\cdot, \\cdot), the distribution after asset choices were made and before income shocks realized.</p> <p>We now write a function to obtain the q (the lotteries) and the indices of a'(y, a)^+, where the masses in the distribution are sent to. Once we got <code>q</code> and <code>indexes</code>, we can use these objects in every forward iteration on D_t.</p> <pre><code>def get_lotteries(policy, grid_a):\n\n    indexes = np.searchsorted(grid_a, policy) # indexes corresponding to a'(y, a)+  (function returns i with a[i-1] &lt; v &lt;= a[i])\n    q = (policy - grid_a[indexes - 1]) / (grid_a[indexes] - grid_a[indexes - 1]) # lotteries\n    return indexes, q\n\n# forward iteration\n@njit\ndef forward_iteration(indexes, q, Pi, D):\n    n_y, n_a = D.shape\n    D_new = np.zeros((n_y, n_a))\n    for y in range(n_y):\n        for a in range(n_a):\n\n            D_new[y, indexes[y, a]]   += q[y, a] * D[y, a]\n            D_new[y, indexes[y, a]-1]     += (1 - q[y, a]) * D[y, a]\n\n    # D_new is D_tilde right now. Now we need to update D_tilde using Pi\n    D_new = Pi @ D_new\n\n    return D_new\n</code></pre> <p>Note that, when calculating <code>D_new = Pi @ D_new</code>, row i_y corresponding to state y will hold the vector </p>  [\\Pi \\tilde D]_{i_y} = (\\pi(i_y, 1), ...,\\pi(i_y, n_y)) \\tilde D = [\\pi(i_y, 1)\\tilde D(1, i_a) + ... +\\pi(i_y, n_y)\\tilde D(n_y, i_a)]_{i_a =1,...,n_a}.  <p>Finally, we compute the steady state distribution by iterating until convergence:</p> <pre><code>def distribution_ss(Pi, policy, grid_a, maxiter=10_000, tol=1E-10, verbose=False):\n\n    indexes, q = get_lotteries(policy, grid_a)\n\n    # initialize distribution\n    D = np.ones_like(policy)/np.size(policy)\n\n    count, error = 0, 1\n    while error &gt; tol and count &lt; maxiter:\n        D_new = forward_iteration(indexes, q, Pi, D)\n        error = np.max(np.abs(D - D_new))\n        D = D_new.copy()\n        count += 1\n\n\n    if verbose : \n        print(\"max |D_t - D_t+1| = \", error, \"\\nnum iterations:\", count)\n\n    return D\n</code></pre> <pre><code>D_ss = distribution_ss(ss.Grids.Pi, ss.a_prime, ss.Grids.grid_a, maxiter=1000, tol=1E-10, verbose=True)\n\n%timeit distribution_ss(ss.Grids.Pi, ss.a_prime, ss.Grids.grid_a, maxiter=1000, tol=1E-10, verbose=True)\n</code></pre>"},{"location":"sem/5-endogenous_grid_method/#using-the-steadystatehh-module","title":"Using the SteadyStateHH Module","text":"<p>Exercise: Add the computation of steady state D to the <code>SteadyStateHH</code> class. Import it from another file and run it.</p> <p>Note: restart the kernel before re-importing the classes. </p> <pre><code>from utils_simm import *\n\ngrid_params = {\n    'n_y' : 7,\n    'n_a' : 500,\n    'min_a' : 0,\n    'max_a' : 10_000\n}\n\nmodel_params = {\n    'beta' : 1-0.08/4, # quarterly discount factor\n    'r' : 0.01/4, # quarterly interest rate\n    'eis' : 1,\n    'rho' : 0.975,\n    'sd_log_y' : 0.7\n}\n\nprint(model_params, \"\\n\",grid_params, sep=\"\")\n# create steady state object\nss = SteadyStateHH(model_params, grid_params)\n</code></pre> <pre><code>{'beta': 0.98, 'r': 0.0025, 'eis': 1, 'rho': 0.975, 'sd_log_y': 0.7}\n{'n_y': 7, 'n_a': 500, 'min_a': 0, 'max_a': 10000}\nthe steady state is unique.\n</code></pre> <pre><code># solve for steady state\nss.solve_ss(ss.model_params)\n\nss.distribution_ss()\n</code></pre> <pre><code>array([[1.40918274e-001, 1.12836882e-004, 7.66408375e-005, ...,\n        4.63400194e-114, 6.22448773e-116, 4.23457364e-118],\n       [1.38212429e-001, 2.53033570e-004, 1.90803709e-004, ...,\n        4.61175517e-114, 6.19620063e-116, 4.21646007e-118],\n       [1.29247539e-001, 6.69139940e-004, 5.20800102e-004, ...,\n        4.61134519e-114, 6.19614397e-116, 4.21685452e-118],\n       ...,\n       [4.76938191e-003, 8.11450720e-005, 5.90862252e-005, ...,\n        4.73711752e-114, 6.35938753e-116, 4.32452420e-118],\n       [1.49636758e-004, 2.56112858e-006, 1.86456018e-006, ...,\n        4.92295035e-114, 6.59920835e-116, 4.48156752e-118],\n       [3.77194925e-006, 6.47532936e-008, 4.71375717e-008, ...,\n        5.27257915e-114, 7.04886178e-116, 4.77491899e-118]])\n</code></pre>"},{"location":"sem/5-endogenous_grid_method/#the-result-utils_simmpy","title":"The result: utils_simm.py","text":"<p>For reference, here is the code for the standard incomplete markets module which we have developed in this notebook.</p> <pre><code>import numpy as np\n\n\nclass Grid:\n\n    def __init__(self, n_y, rho, sd_log_y, n_a, min_a, max_a):\n        self.n_y = n_y\n        self.rho = rho\n        self.sd_log_y = sd_log_y\n        self.n_a = n_a\n        self.min_a = min_a\n        self.max_a = max_a\n        self.Pi, self.grid_y = self.rouwenhorst(n_y, rho, sd_log_y)\n        self.pi_ss = self.stationary_dist(self.Pi)[0,:]\n        self.grid_y = self.normalize_y(self.grid_y, self.pi_ss)\n        self.grid_a = self.discretize_assets(min_a, max_a, n_a)\n\n    # sigma is the sd of the error, e_t\n    def rouwenhorst(self, n, rho, sd_log_y):\n\n        # the grid    \n        e = np.arange(n) # sd of e on this grid with Pi is sqrt(n-1)/2\n        e = e / ( (n-1)**0.5 /2 ) # now its unit sd\n        e = e * sd_log_y # now it's the sd of the cross section of log_y\n\n        # the transition matrix\n        p = (1+rho)/2\n        Pi = np.array([[p, 1-p], [1-p, p]])\n\n        while Pi.shape[0] &lt; n:\n            Pi_next = np.zeros((1+Pi.shape[0], 1+Pi.shape[1]))\n            Pi_next[0:Pi.shape[0], 0:Pi.shape[1]] += Pi * p\n            Pi_next[0:Pi.shape[0], -Pi.shape[1]:] += Pi * (1-p)\n            Pi_next[-Pi.shape[0]:, -Pi.shape[1]:] += Pi * p\n            Pi_next[-Pi.shape[0]:, 0:Pi.shape[1]] += Pi * (1-p)\n            Pi_next[1:-1, :] /= 2\n            Pi = Pi_next\n\n        return Pi, e\n\n    def stationary_dist(self, Pi):\n        Pi_stationary = Pi.copy()\n        eps = 1\n        while eps &gt; 10E-12:\n            Pi_old = Pi_stationary.copy()\n            Pi_stationary = Pi_stationary @ Pi_stationary\n            eps = np.max(np.abs(Pi_stationary - Pi_old))\n\n        if np.max(\n                np.abs( \n                    np.sum(Pi_stationary - Pi_stationary,axis = 0) / Pi_stationary.shape[0]\n                )\n            ) &lt; 10E-10:\n            print(\"the steady state is unique.\")\n\n        return Pi_stationary\n\n    def normalize_y(self, log_y, pi_ss): # make y have unit mean\n        y = np.exp(log_y)\n        y = y / np.vdot(y, pi_ss)\n        return y\n\n\n    # write a function which discretizes the asset space\n    def discretize_assets(self, amin, amax, n_a):\n        # find ubar \n        ubar = np.log(np.log(amax - amin + 1)+1)\n        # make linar grid for the u's\n        grid_u = np.linspace(0,ubar, n_a)\n        # transform back to a\n        grid_a = amin + np.exp(np.exp(grid_u)-1)-1\n        return grid_a\n\n\nfrom numba import njit\n\n@njit\ndef backward_iteration(V_a, beta, eis, r, grid_a, grid_y, Pi):\n    W = beta * Pi @ V_a\n\n    c_endog = W ** (-1/eis)\n    a_endog = (1 + r)**(-1) * (grid_a[np.newaxis, :] - grid_y[:, np.newaxis] + c_endog)\n    a_prime = np.empty((grid_y.shape[0], grid_a.shape[0]))\n\n    for i_y in range(grid_y.shape[0]):\n        a_prime[i_y, :] = np.interp(grid_a, a_endog[i_y, :], grid_a)\n\n    c = (1 + r) * (grid_a[np.newaxis, :] + grid_y[:, np.newaxis]) - a_prime\n\n    return c, a_prime\n\ndef get_lotteries(policy, grid_a):\n\n    indexes = np.searchsorted(grid_a, policy) # indexes corresponding to a'(y, a)+  (function returns i with a[i-1] &lt; v &lt;= a[i])\n    q = (policy - grid_a[indexes - 1]) / (grid_a[indexes] - grid_a[indexes - 1]) # lotteries\n    return indexes, q\n\n# forward iteration\n@njit\ndef forward_iteration(indexes, q, Pi, D):\n    n_y, n_a = D.shape\n    D_new = np.zeros((n_y, n_a))\n    for y in range(n_y):\n        for a in range(n_a):\n\n            D_new[y, indexes[y, a]]   += q[y, a] * D[y, a]\n            D_new[y, indexes[y, a]-1]     += (1 - q[y, a]) * D[y, a]\n\n    # D_new is D_tilde right now. Now we need to update D_tilde using Pi\n    D_new = Pi @ D_new\n\n    return D_new\n\nclass SteadyStateHH:\n\n    def __init__(self, model_params, grid_params, tol = 1e-6, max_iter = 1_000):\n        self.model_params = model_params\n        self.grid_params = grid_params\n        self.Grids = Grid(n_y = grid_params['n_y'], rho = model_params['rho'],  sd_log_y = model_params['sd_log_y'], n_a = grid_params['n_a'], min_a = grid_params['min_a'], max_a = grid_params['max_a'])\n        self.tol = tol\n        self.max_iter = max_iter\n        self.c = None\n        self.a_prime = None\n        self.V_a = None\n        self.D = None\n\n    # adding the model_params as an argument allows solving for different parameterizations\n    def solve_ss(self, model_params):\n\n        # update grid if necessary\n        if (self.model_params['rho'], self.model_params['sd_log_y']) != (model_params['rho'], model_params['sd_log_y']):\n            self.Grids = Grid(n_y = self.grid_params['n_y'], rho = model_params['rho'],  sd_log_y = model_params['sd_log_y'], n_a = self.grid_params['n_a'], min_a = self.grid_params['min_a'], max_a = self.grid_params['max_a'])\n        # update model_params if necessary\n        if self.model_params != model_params:\n            self.model_params = model_params\n\n        # initialize value function derivative with guess\n        if self.V_a is None:\n            V_a = np.ones((self.grid_params['n_y'], self.grid_params['n_a']))\n        else:\n            V_a = self.V_a\n\n        for i in range(self.max_iter):\n            c, a_prime = backward_iteration(V_a, model_params['beta'], model_params['eis'], model_params['r'], self.Grids.grid_a, self.Grids.grid_y, self.Grids.Pi)\n            V_a_new = (1 + model_params['r']) * c **(-1/model_params['eis'])\n\n            if np.max(np.abs(V_a_new - V_a)) &lt; self.tol:\n                break\n\n            V_a = V_a_new\n\n        self.c = c\n        self.a_prime = a_prime\n        self.V_a = V_a\n\n        return c, a_prime\n\n    def distribution_ss(self, maxiter=10_000, tol=1E-10, verbose=False):\n\n        assert self.a_prime is not None, \"solve_ss must be called first\"\n\n        Pi = self.Grids.Pi\n        grid_a = self.Grids.grid_a\n        policy = self.a_prime\n\n        indexes, q = get_lotteries(policy, grid_a)\n\n        # initialize distribution\n        D = np.ones_like(policy)/np.size(policy)\n\n        count, error = 0, 1\n        while error &gt; tol and count &lt; maxiter:\n            D_new = forward_iteration(indexes, q, Pi, D)\n            error = np.max(np.abs(D - D_new))\n            D = D_new.copy()\n            count += 1\n\n\n        if verbose : \n            print(\"max |D_t - D_t+1| = \", error, \"\\nnum iterations:\", count)\n\n        self.D = D\n\n        return D\n\n    def plot_policy(self, bound_grid = 0.4):\n        \"\"\" \n        Plot the policy function for the first 4 income states\n        bound_grid: float, fraction of the grid to plot\n        \"\"\"\n        rng_asset_grid = int(grid_params['n_a']*bound_grid)\n        fig, ax = plt.subplots()\n        for i_y, y in enumerate(self.Grids.grid_y[0:4]):\n            ax.plot(self.Grids.grid_a[0:rng_asset_grid], self.c[i_y, 0:rng_asset_grid], label = f'y = {y:.2f}')\n        ax.set(xlabel = r'$a$', ylabel = r'$c(y,a)$', title = 'Steady State Policy Function')\n        plt.legend(fontsize = 'small')\n        plt.show()\n</code></pre>"}]}